{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janchorowski/dl_uwr/blob/summer2021/Assignments/Assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRC-uwC711Xf"
      },
      "source": [
        "# Assignment 5\n",
        "\n",
        "**Submission deadlines:**\n",
        "- get at least 4 points by Tuesday, 8.06 \n",
        "- remaining points: last lab session of the semester\n",
        "\n",
        "**Points:** Aim to get 12 out of 16 possible points.\n",
        "\n",
        "\n",
        "## Submission instructions\n",
        "The class is held remotely. To submit your solutions please show the notebook over the video call. Make sure you know all the questions and answers, and that the notebook contains results (before presentation do `Runtime -> Restart and run all`)\n",
        "\n",
        "We provide a starter code, however, you are not required to use it as long as you properly solve the tasks.\n",
        "\n",
        "As always, please submit corrections using GitHub's Pull Requests to https://github.com/janchorowski/dl_uwr."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpNiU4qgCyvG"
      },
      "source": [
        "# Problem 1: Generating 2 Moons data\n",
        "Consider the 2D moons dataset generated in the starter code below. \n",
        "\n",
        "Train the following models: VAE, GAN, RealNVP. \n",
        "\n",
        "For each model visualize how the latent space is transformed into data points. Train a model with 2D latent space. Then whenever applicable:\n",
        "1. Plot the data points along with a grid, then plot latent encodings of the data and the grid.\n",
        "2. Generate latent encodings and a grid in latent space. Then sample data points based on the generated latent space and grid. Plot them in the data space.\n",
        "\n",
        "Hint: for better visual mapping between original and latent representations of the data, use color on the scatterplots.\n",
        "\n",
        "## Tasks:\n",
        "1. [4p] Variational Autoencoder (VAE)\n",
        "\n",
        "  Please use the following probability distributions:\n",
        "    \n",
        "  $$\n",
        "    \\begin{split}\n",
        "    p(z) &= \\mathcal{N}(z; \\mu=0, \\sigma^2=1)  \\\\\n",
        "    p(x | z) &= \\mathcal{N}(x; \\mu=x_{\\mu}(z), \\sigma^2=x_{\\sigma^2})\n",
        "    \\end{split}\n",
        "  $$\n",
        "\n",
        "  where: $x_{\\mu}(z)$ is computed by a small neural network and $x_{\\sigma^2}$ is a learned parameter.\n",
        "\n",
        "  **Answer these questions:**\n",
        "  \n",
        "  1. Write down the formula for the reconstruction loss. The variance appears in two terms, what is their function?\n",
        "\n",
        "  2. The reconstruction loss may fall below zero. How is this possible? \n",
        "\n",
        "  Compare the operation of the VAE for 2 Moons with a few widths. How is the loss dependent on the width? Why?\n",
        "\n",
        "2. [3p] Generative Adversarial Network (GAN)\n",
        "\n",
        "  Please use either a standard normal distribution for the latent space or a uniform distribution.\n",
        "\n",
        "  **Answer these questions:**\n",
        "  - Is the GAN generating all possible samples? Is it concentrating more on some regions of the data space?\n",
        "\n",
        "3. [3p] Normalizing Flow (Real NVP model).\n",
        "\n",
        "  The goal of Normalizing Flows is to learn invertible transformation between complex data distribution and some simple one, i.e. multivariate isotropic Gaussian. Since the latter is continuous, and our dataset is finite (hence in fact forms a discrete distribution, because you can memorize all observed values), we need to perform *dequantization*, that is add random noise to data at every iteration to train the model on an infinite stream of data. This is especially important in Problem 2 when RealNVP is applied to the MNIST data.\n",
        "\n",
        "    **Answer these questions:**\n",
        "  - Scale the input data by 0.01. Why is the negative log-likelihood negative?\n",
        "  - For a few 2 moon widths compare the negative log-likelihood of the data with the values obtained with the VAE and present results in a table. How do they compare? Please note, that GAN's do not return likelihood computation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT0EIrkQ3Wyj"
      },
      "source": [
        "# Problem 2: Generating MNIST\n",
        "\n",
        "Next, choose one of the generative models (VAE, RealNVP, or GAN) and do the following using the MNIST dataset:\n",
        "\n",
        "##Tasks\n",
        "1. [2p] Train the model. Visualize generated samples (and test/train data reconstructions if you chose VAE).\n",
        "\n",
        "Please properly define $p(x|z)$: when using a VAE, you can binarize the data and predict binary variables. For RealNVP, you can normalize the data to the 0-1 range, and add uniform noise $[0,1/255]$ to dequantize intensity values. GAN does not assume an output probability distribution and you may use raw data.\n",
        "\n",
        "2. [1p] Linearly interpolate between two points in the latent space. Generate images from them and intermediate points. \n",
        "\n",
        "3. [1p] Try a different interpolation method, i.e. instead of going in a straight line from one point to another, follow the spherical curve of prior Gaussian distribution.\n",
        "\n",
        "4. [2p] It is sometimes desirable to make use of available labels to gain control over classes of generated samples. Create a conditional generative model of your choice by injecting one-hot embeddings into the proper component(s). An example is [Conditional GAN](https://arxiv.org/abs/1411.1784), and its idea is applicable to other methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPj2Nbe53Ftq"
      },
      "source": [
        "# Starter code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUw5W427wF15"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import MultivariateNormal, Uniform\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "device = \"cuda\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03R6TglEGmef"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U-VDPrXwUkf"
      },
      "source": [
        "def generate_moons(width=1.0):\n",
        "    moon1 = [\n",
        "        [r * np.cos(a) - 2.5, r * np.sin(a) - 1.0]\n",
        "        for r in np.arange(5 - width, 5 + width, 0.1 * width)\n",
        "        for a in np.arange(0, np.pi, 0.01)\n",
        "    ]\n",
        "    moon2 = [\n",
        "        [r * np.cos(a) + 2.5, r * np.sin(a) + 1.0]\n",
        "        for r in np.arange(5 - width, 5 + width, 0.1 * width)\n",
        "        for a in np.arange(np.pi, 2 * np.pi, 0.01)\n",
        "    ]\n",
        "    points = torch.tensor(moon1 + moon2)\n",
        "    points += torch.rand(points.shape) * width\n",
        "    return points.float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_-2d0LYacVP"
      },
      "source": [
        "class InMemDataLoader(object):\n",
        "    __initialized = False\n",
        "    def __init__(self, tensors, batch_size=1, shuffle=False, sampler=None,\n",
        "                 batch_sampler=None, drop_last=False):\n",
        "        \"\"\"A torch dataloader that fetches data from memory.\"\"\"\n",
        "        tensors = [torch.tensor(tensor) for tensor in tensors]\n",
        "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.drop_last = drop_last\n",
        "        \n",
        "        if batch_sampler is not None:\n",
        "            if batch_size > 1 or shuffle or sampler is not None or drop_last:\n",
        "                raise ValueError('batch_sampler option is mutually exclusive '\n",
        "                                 'with batch_size, shuffle, sampler, and '\n",
        "                                 'drop_last')\n",
        "            self.batch_size = None\n",
        "            self.drop_last = None\n",
        "\n",
        "        if sampler is not None and shuffle:\n",
        "            raise ValueError('sampler option is mutually exclusive with '\n",
        "                             'shuffle')\n",
        "            \n",
        "        if batch_sampler is None:\n",
        "            if sampler is None:\n",
        "                if shuffle:\n",
        "                    sampler = torch.utils.data.RandomSampler(dataset)\n",
        "                else:\n",
        "                    sampler = torch.utils.data.SequentialSampler(dataset)\n",
        "            batch_sampler = torch.utils.data.BatchSampler(sampler, batch_size, drop_last)\n",
        "\n",
        "        self.sampler = sampler\n",
        "        self.batch_sampler = batch_sampler\n",
        "        self.__initialized = True\n",
        "    \n",
        "    def __setattr__(self, attr, val):\n",
        "        if self.__initialized and attr in ('batch_size', 'sampler', 'drop_last'):\n",
        "            raise ValueError('{} attribute should not be set after {} is '\n",
        "                             'initialized'.format(attr, self.__class__.__name__))\n",
        "\n",
        "        super(InMemDataLoader, self).__setattr__(attr, val)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch_indices in self.batch_sampler:\n",
        "            yield self.dataset[batch_indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch_sampler)\n",
        "    \n",
        "    def to(self, device):\n",
        "        self.dataset.tensors = tuple(t.to(device) for t in self.dataset.tensors)\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf1xiHEy2tj3"
      },
      "source": [
        "moons = generate_moons(width=1.0)\n",
        "moons_dl = InMemDataLoader([moons], batch_size=2048, shuffle=True)\n",
        "moons_dl.to(device)\n",
        "plt.scatter(moons[:, 0], moons[:, 1], s=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZaXjFb0Gknw"
      },
      "source": [
        "## VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "depZip50Gnfp"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, in_dim=2, hid_dim=128, z_dim=2):\n",
        "        super(VAE, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hid_dim, hid_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hid_dim, 2 * z_dim),\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, hid_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hid_dim, hid_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hid_dim, in_dim),\n",
        "        )\n",
        "\n",
        "        self.x_log_var = torch.nn.Parameter(torch.zeros(in_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        sampled_z, z_mu, z_log_var = self.encode(x)\n",
        "        x_mu, x_log_var = self.decode(sampled_z)\n",
        "        return x_mu, x_log_var, z_mu, z_log_var\n",
        "\n",
        "    def encode(self, x):\n",
        "        # Implement encoding procedure.\n",
        "        # First, get z_mu and z_log_var from the encoder.\n",
        "        # Second, compute z samples using the  the reparametrization trick.\n",
        "        # TODO\n",
        "        z_mu_log_var = self.encoder(x)\n",
        "        z_mu, z_log_var = torch.chunk(z_mu_log_var, 2, dim=1)\n",
        "        sampled_z = TODO\n",
        "        return sampled_z, z_mu, z_log_var\n",
        "\n",
        "    def decode(self, sampled_z):\n",
        "        x_mu = self.decoder(sampled_z)\n",
        "        return x_mu, self.x_log_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4m8bjOXLPCy"
      },
      "source": [
        "def gaussian_negative_log_likelihood(x, mu, log_var):\n",
        "    \"\"\"Compute N(x; mu=mu, sigma^2=exp(log_var)^2).\"\"\"\n",
        "    return TODO\n",
        "\n",
        "\n",
        "def kullback_leibler_divergence_between_two_gaussians(mu1, log_var1, mu2, log_var2):\n",
        "    return (\n",
        "        log_var2 - log_var1 + (log_var1.exp() + (mu1 - mu2) ** 2) / (log_var2.exp()) - 1\n",
        "    ) / 2\n",
        "\n",
        "\n",
        "def reconstruction_loss(predicted_x_mu, predicted_x_log_var, real_x):\n",
        "    \"\"\"Return the negtive log-likelihood reconstruction loss:\n",
        "        1/N \\sum_i=1^N -\\log N(x_i ; predicted_x_mu, I * exp(predicted_x_log_var)^2)\n",
        "    \"\"\"\n",
        "    return TODO\n",
        "\n",
        "\n",
        "def kl_loss(z_mu, z_log_var):\n",
        "    \"\"\"Compute Kullback–Leibler divergence between N(z_mu, exp(z_log_var)^2) and N(0,1).\n",
        "\n",
        "    \"\"\"\n",
        "    return TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o-ERw8E24yK"
      },
      "source": [
        "hid_dim = 64\n",
        "z_dim = 2\n",
        "lr = 0.0003\n",
        "\n",
        "vae = VAE(hid_dim=hid_dim, z_dim=z_dim).to(device)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaTT2l073cQA"
      },
      "source": [
        "# Please note: the model should converge faster!\n",
        "for i in range(7500):\n",
        "    recon_loss_acc = 0.0\n",
        "    kl_acc = 0.0\n",
        "    vae.train()\n",
        "    for x, in moons_dl:\n",
        "        x = x.float().to(device)\n",
        "\n",
        "        x_mu, x_log_var, z_mu, z_log_var = vae(x)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        recon_loss = reconstruction_loss(x_mu, x_log_var, x)\n",
        "        kl = kl_loss(z_mu, z_log_var)\n",
        "        loss = recon_loss + kl\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        recon_loss_acc += recon_loss.item() * len(x)\n",
        "        kl_acc += kl.item() * len(x)\n",
        "\n",
        "    if i % 200 == 0:\n",
        "        print(\n",
        "            f\"Epoch: {i} loss: {(recon_loss_acc + kl_acc) / len(moons) :.4f} recon_loss: {recon_loss_acc / len(moons) :.4f} kl_loss: {kl_acc / len(moons) :.4f} avg mean: {z_mu.detach().mean() :.4f} avg std: {torch.exp(z_log_var.detach() / 2).mean() :.4f}\"\n",
        "        )\n",
        "        vae.eval()\n",
        "        with torch.no_grad():\n",
        "            # Reconstruct data\n",
        "            x_recon = torch.randn(x.shape).to(device) * torch.exp(x_log_var / 2) + x_mu\n",
        "            x_recon = x_recon.cpu()\n",
        "\n",
        "            plt.scatter(x_recon[:, 0], x_recon[:, 1])\n",
        "            plt.title(\"Reconstruction\")\n",
        "            plt.show()\n",
        "\n",
        "            # Generate new data\n",
        "            z = torch.randn(500, z_dim).to(device)\n",
        "            x_gen_mu, x_gen_log_var = vae.decode(z)\n",
        "            x_gen = (\n",
        "                torch.randn(z.shape[0], 2).to(device) * torch.exp(x_gen_log_var / 2)\n",
        "                + x_gen_mu\n",
        "            )\n",
        "            x_gen = x_gen.cpu()\n",
        "\n",
        "            plt.scatter(x_gen[:, 0], x_gen[:, 1])\n",
        "            plt.title(\"Generation\")\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NIA-xRP9mim"
      },
      "source": [
        "def get_grid(data):\n",
        "    \"\"\"Generate a dataset of points that lie on grid and span the given data range.\"\"\"\n",
        "\n",
        "    xmin, xmax = np.floor(data.min(0)), np.ceil(data.max(0))\n",
        "    xg, yg = np.meshgrid(\n",
        "        np.arange(xmin[0], xmax[0] + 1, 1), np.arange(xmin[1], xmax[1] + 1, 1)\n",
        "    )\n",
        "    mxg = np.hstack(\n",
        "        (\n",
        "            np.hstack((xg, np.zeros((xg.shape[0], 1)) + np.nan)).ravel(),\n",
        "            np.hstack((xg.T, np.zeros((xg.shape[1], 1)) + np.nan)).ravel(),\n",
        "        )\n",
        "    )\n",
        "    myg = np.hstack(\n",
        "        (\n",
        "            np.hstack((yg, np.zeros((yg.shape[0], 1)) + np.nan)).ravel(),\n",
        "            np.hstack((yg.T, np.zeros((yg.shape[1], 1)) + np.nan)).ravel(),\n",
        "        )\n",
        "    )\n",
        "    grid = np.vstack((mxg, myg)).T\n",
        "    return grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mRtRod68qkL"
      },
      "source": [
        "data = np.array(moons)[np.random.permutation(moons.shape[0])[:1000]]\n",
        "grid = get_grid(data)\n",
        "\n",
        "data_colors = (data[:, 0] - min(data[:, 0])) / (max(data[:, 0]) - min(data[:, 0]))\n",
        "data_colors = plt.cm.jet(data_colors)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.plot(grid[:, 0], grid[:, 1], color=\"gray\", alpha=0.3)\n",
        "plt.scatter(data[:, 0], data[:, 1], color=data_colors, s=1.0)\n",
        "_ = plt.axis(\"equal\")\n",
        "plt.title(\"Data in original space\")\n",
        "\n",
        "vae.eval()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "# TODO: compute the latent encoding of the data nd the grid\n",
        "enc_grid = TODO\n",
        "enc_data = TODO\n",
        "\n",
        "plt.plot(enc_grid[:, 0], enc_grid[:, 1], color=\"gray\", alpha=0.3)\n",
        "plt.scatter(enc_data[:, 0], enc_data[:, 1], color=data_colors, s=1.0)\n",
        "_ = plt.axis(\"equal\")\n",
        "plt.title(\"Data in latent space\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB_zss4NABEK"
      },
      "source": [
        "latent_samples = torch.randn(1000, z_dim)\n",
        "\n",
        "latent_colors = (latent_samples[:, 0] - min(latent_samples[:, 0])) / (\n",
        "    max(latent_samples[:, 0]) - min(latent_samples[:, 0])\n",
        ")\n",
        "latent_colors = plt.cm.jet(latent_colors.numpy())\n",
        "\n",
        "latent_grid = get_grid(latent_samples.numpy())\n",
        "\n",
        "vae.eval()\n",
        "\n",
        "# TODO: compute the projection into data space of the latent saples and the grid\n",
        "x_gen = TODO\n",
        "grid_gen = TODO\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(latent_grid[:, 0], latent_grid[:, 1], color=\"gray\", alpha=0.3)\n",
        "plt.scatter(latent_samples[:, 0], latent_samples[:, 1], color=latent_colors, s=1)\n",
        "_ = plt.axis(\"equal\")\n",
        "plt.title(\"Z in latent space\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "plt.plot(grid_gen[:, 0], grid_gen[:, 1], color=\"gray\", alpha=0.3)\n",
        "plt.scatter(x_gen[:, 0], x_gen[:, 1], color=latent_colors, s=1)\n",
        "_ = plt.axis(\"equal\")\n",
        "plt.title(\"Generated data in original space\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yosfsSTGhyw"
      },
      "source": [
        "## GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz36Jq6hQzqs"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_dim=2, hid_dim=128, out_dim=2):\n",
        "        super(Generator, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hid_dim, hid_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hid_dim, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_dim=2, hid_dim=128, out_dim=1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hid_dim, hid_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hid_dim, out_dim),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsJT3DFWAzmY"
      },
      "source": [
        "def generator_loss(DG, eps=1e-6):\n",
        "    # Define Generator loss. Use eps for numerical stability of log.\n",
        "    return TODO\n",
        "\n",
        "\n",
        "def discriminator_loss(DR, DG, eps=1e-6):\n",
        "    # Define Discriminator loss. Use eps for numerical stability of log.\n",
        "    return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXMleqaUA5FQ"
      },
      "source": [
        "z_dim = 2\n",
        "hid_dim = 64\n",
        "lr = 0.0001\n",
        "\n",
        "G = Generator(in_dim=z_dim, hid_dim=hid_dim).to(device)\n",
        "D = Discriminator(hid_dim=hid_dim).to(device)\n",
        "G_optimizer = optim.Adam(G.parameters(), lr=lr)\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ggBMd88D_By"
      },
      "source": [
        "for i in range(7500):\n",
        "    G_loss_acc = 0.0\n",
        "    D_loss_acc = 0.0\n",
        "    G.train()\n",
        "    D.train()\n",
        "    for x, in moons_dl:\n",
        "        x = x.float().to(device)\n",
        "\n",
        "        # Generate fake data from z ~ N(0,1).\n",
        "        # Calculate Generator loss.\n",
        "        z = torch.randn(x.size(0), z_dim, device=device)\n",
        "        x_fake = TODO  # Use the generator to compute x_Fake\n",
        "\n",
        "        # make a copy of x_fake and detach it, we'll use the copy to train the Discriminator\n",
        "        x_fake_detached = x_fake.detach()\n",
        "\n",
        "        G_loss = TODO  # Now use the discriminator and compute generator loss\n",
        "\n",
        "        G_optimizer.zero_grad()\n",
        "        G_loss.backward()\n",
        "        G_optimizer.step()\n",
        "\n",
        "        # Calculate Discriminator loss.\n",
        "        # Remember to use x_fake_detached to prevent backpropagating through generator!\n",
        "        D_loss=\n",
        "\n",
        "        D_optimizer.zero_grad()\n",
        "        D_loss.backward()\n",
        "        D_optimizer.step()\n",
        "\n",
        "        G_loss_acc += G_loss.item() * len(x)\n",
        "        D_loss_acc += D_loss.item() * len(x)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        G.eval()\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(1000, z_dim, device=device)\n",
        "            x_gen = G(z).cpu()\n",
        "            plt.scatter(x_gen[:, 0], x_gen[:, 1])\n",
        "            plt.title(\n",
        "                f\"Epoch: {i} Generator loss: {G_loss_acc / len(moons) :.4f} Discriminator loss: {D_loss_acc / len(moons) :.4f}\"\n",
        "            )\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul1Q829FNk8E"
      },
      "source": [
        "latent_samples = torch.randn(1000, z_dim)\n",
        "\n",
        "latent_colors = (latent_samples[:, 0] - min(latent_samples[:, 0])) / (\n",
        "    max(latent_samples[:, 0]) - min(latent_samples[:, 0])\n",
        ")\n",
        "latent_colors = plt.cm.jet(latent_colors.numpy())\n",
        "\n",
        "latent_grid = get_grid(latent_samples.numpy())\n",
        "\n",
        "G.eval()\n",
        "# TODO: compute the projection into data space of the latent saples and the grid\n",
        "x_gen = TODO\n",
        "grid_gen = TODO\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(latent_grid[:, 0], latent_grid[:, 1], color=\"gray\", alpha=0.3)\n",
        "plt.scatter(latent_samples[:, 0], latent_samples[:, 1], color=latent_colors, s=1)\n",
        "_ = plt.axis(\"equal\")\n",
        "plt.title(\"Z in latent space\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "plt.plot(grid_gen[:, 0], grid_gen[:, 1], color=\"gray\", alpha=0.3)\n",
        "plt.scatter(x_gen[:, 0], x_gen[:, 1], color=latent_colors, s=1)\n",
        "_ = plt.axis(\"equal\")\n",
        "plt.title(\"Generated data in original space\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wxleEkAMcT1"
      },
      "source": [
        "## Normalizing Flow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGDuE2H0MbrR"
      },
      "source": [
        "class CouplingLayer(nn.Module):\n",
        "    def __init__(self, idx, in_dim, hid_dim, out_dim):\n",
        "        super(CouplingLayer, self).__init__()\n",
        "        self.in_dim = in_dim // 2\n",
        "        self.hid_dim = hid_dim\n",
        "        self.out_dim = out_dim // 2\n",
        "        self.odd = idx % 2 == 1\n",
        "\n",
        "        self.s = nn.Sequential(\n",
        "            nn.Linear(self.in_dim, self.hid_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hid_dim, self.out_dim),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        self.t = nn.Sequential(\n",
        "            nn.Linear(self.in_dim, self.hid_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hid_dim, self.out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, ldetJ):\n",
        "        # Split x into two halves along dimension axis.\n",
        "        x1, x2 = x[:, 0, None], x[:, 1, None]  # x1, x2 = TODO\n",
        "        if self.odd:\n",
        "            x1, x2 = x2, x1\n",
        "\n",
        "        s = self.s(x1)\n",
        "        t = self.t(x1)\n",
        "        # Transform x2 using s and t.\n",
        "        # x1 = x1\n",
        "        x2 = TODO\n",
        "\n",
        "        if self.odd:\n",
        "            x1, x2 = x2, x1\n",
        "\n",
        "        # Add log determinant of the Jacobian\n",
        "        ldetJ += TODO\n",
        "\n",
        "        return torch.cat([x1, x2], dim=1), ldetJ\n",
        "\n",
        "    def invert(self, z):\n",
        "        z1, z2 = z[:, 0, None], z[:, 1, None]\n",
        "        if self.odd:\n",
        "            z1, z2 = z2, z1\n",
        "\n",
        "        s = self.s(z1)\n",
        "        t = self.t(z1)\n",
        "        # Invert the transformation of x2 from the forward step.\n",
        "        # z1 = z1\n",
        "        z2 = TODO\n",
        "\n",
        "        if self.odd:\n",
        "            z1, z2 = z2, z1\n",
        "\n",
        "        return torch.cat([z1, z2], dim=1)\n",
        "\n",
        "\n",
        "class RealNVP(nn.Module):\n",
        "    def __init__(self, n_coupling_layers=4, in_dim=2, hid_dim=128, out_dim=2):\n",
        "        super(RealNVP, self).__init__()\n",
        "        assert n_coupling_layers % 2 == 0\n",
        "        self.in_dim = in_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        self.coupling_layers = nn.ModuleList(\n",
        "            [\n",
        "                CouplingLayer(i, in_dim, hid_dim, out_dim)\n",
        "                for i in range(n_coupling_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        ldetJ = 0.0\n",
        "        for layer in self.coupling_layers:\n",
        "            x, ldetJ = layer(x, ldetJ)\n",
        "        return x, ldetJ\n",
        "\n",
        "    def invert(self, z):\n",
        "        for layer in reversed(self.coupling_layers):\n",
        "            z = layer.invert(z)\n",
        "        return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmOqRaako4vF"
      },
      "source": [
        "def nll_loss(z, ldetJ, prior_z):\n",
        "    # Define negative log-likelihood loss for change of variable formula.\n",
        "    # Hint: use prior_z.log_prob().\n",
        "    return TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTzk7Avxp7Vm"
      },
      "source": [
        "n_coupling_layers = 6\n",
        "hid_dim = 64\n",
        "lr = 0.003\n",
        "\n",
        "flow = RealNVP(n_coupling_layers=n_coupling_layers, hid_dim=hid_dim).to(device)\n",
        "optimizer = optim.Adam(flow.parameters(), lr=lr)\n",
        "prior_z = MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdHXBe9pqNnD"
      },
      "source": [
        "for i in range(1000):\n",
        "    loss_acc = 0.0\n",
        "    flow.train()\n",
        "    for x, in moons_dl:\n",
        "        x = x.float().to(device)\n",
        "        # dequantization - add a little random noise\n",
        "        x += torch.rand(x.shape, device=device) / 1e2\n",
        "        z, ldetJ = flow(x)\n",
        "\n",
        "        loss = nll_loss(z, ldetJ, prior_z)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_acc += loss.item() * len(x)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        flow.eval()\n",
        "        with torch.no_grad():\n",
        "            z = prior_z.sample((1000,))\n",
        "            x = flow.invert(z).cpu()\n",
        "            plt.scatter(x[:, 0], x[:, 1])\n",
        "            plt.title(f\"Epoch: {i} nll loss: {loss_acc / len(moons) :.4f}\")\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2p8mr1fRtZK"
      },
      "source": [
        "data = np.array(moons)[np.random.permutation(moons.shape[0])[:1000]]\n",
        "grid = get_grid(data)\n",
        "\n",
        "data_colors = (data[:, 0] - min(data[:, 0])) / (max(data[:, 0]) - min(data[:, 0]))\n",
        "data_colors = plt.cm.jet(data_colors)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.plot(grid[:, 0], grid[:, 1], color=\"gray\", alpha=0.3)\n",
        "plt.scatter(data[:, 0], data[:, 1], color=data_colors, s=1.0)\n",
        "_ = plt.axis(\"equal\")\n",
        "plt.title(\"Data in original space\")\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "# TODO: compute the latent encoding of the data nd the grid\n",
        "enc_grid = TODO\n",
        "enc_data = TODO\n",
        "\n",
        "plt.plot(enc_grid[:, 0], enc_grid[:, 1], color=\"gray\", alpha=0.3)\n",
        "plt.scatter(enc_data[:, 0], enc_data[:, 1], color=data_colors, s=1.0)\n",
        "_ = plt.axis(\"equal\")\n",
        "plt.title(\"Data in latent space\")\n",
        "plt.xlim(-5, 5)\n",
        "plt.ylim(-5, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65snuXdRRtZ3"
      },
      "source": [
        "latent_samples = prior_z.sample((1000,)).cpu()\n",
        "\n",
        "latent_colors = (latent_samples[:, 0] - min(latent_samples[:, 0])) / (\n",
        "    max(latent_samples[:, 0]) - min(latent_samples[:, 0])\n",
        ")\n",
        "latent_colors = plt.cm.jet(latent_colors.numpy())\n",
        "\n",
        "latent_grid = get_grid(latent_samples.numpy())\n",
        "\n",
        "# TODO: compute the projection into data space of the latent saples and the grid\n",
        "x_gen = TODO\n",
        "grid_gen = TODO\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(latent_grid[:, 0], latent_grid[:, 1], color=\"gray\", alpha=0.3)\n",
        "plt.scatter(latent_samples[:, 0], latent_samples[:, 1], color=latent_colors, s=1)\n",
        "_ = plt.axis(\"equal\")\n",
        "plt.title(\"Z in latent space\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "plt.plot(grid_gen[:, 0], grid_gen[:, 1], color=\"gray\", alpha=0.3)\n",
        "plt.scatter(x_gen[:, 0], x_gen[:, 1], color=latent_colors, s=1)\n",
        "_ = plt.axis(\"equal\")\n",
        "plt.title(\"Generated data in original space\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt4NDc_S3P7m"
      },
      "source": [
        "## MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_Mwv74eJZnZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}