{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/janchorowski/dl_uwr/blob/summer2021/Assignments/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGXgWugfJ0Vl"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "**Submission deadlines:** \n",
    "- get at least 4 points by Tuesday, 9.03.2021\n",
    "- remaining points: last lab session before or on Tuesday, 16.03.2021\n",
    "\n",
    "**Points:** Aim to get 8 out of 12 possible points\n",
    "\n",
    "## Submission instructions\n",
    "The class is held remotely. To sumbmit your solutions please show the notebook over the video call. Make sure you know all the questions and asnwers, and that the notebook contains results (before presentation do `Runtime -> Restar and run all`)\n",
    "\n",
    "We provide starter code, however you are not required to use it as long as you properly solve the tasks.\n",
    "\n",
    "As always, please submit corrections using GitHub's Pull Requests to https://github.com/janchorowski/dl_uwr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5S8iRaCPyO2a"
   },
   "source": [
    "# Task description\n",
    "\n",
    "## TLDR\n",
    "Implement and train a neural network using pure numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHcz7I2V-bVM"
   },
   "source": [
    "\n",
    "## Problem 1 [2p]\n",
    "Implement a two-layer network, manually set weights and biases to solve the XOR task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QSpZxuH-bLe"
   },
   "source": [
    "## Problem 2 [2p]\n",
    "1. Add a backward pass.\n",
    "2. Use a sensible random initialization for weights and biases.\n",
    "3. Numerically check the correctness of your gradient computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1Tn8j0m-bAy"
   },
   "source": [
    "## Problem 3 [2p]\n",
    "1. Implement gradient descent\n",
    "2. Train your network to solve 3D XOR\n",
    "3. Try several hidden layer sizes, for each size record the fracton of successful trainings. Then answer:\n",
    "    - What is the minimal hidden size required to solve 3D XOR (even with low reliability, when the training has to be repeated multiple times)\n",
    "    - What is the minimal hidden size required to reliably solve 3D XOR\n",
    "    - Which networks are easier to train - small or lare ones? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP9Pvpmf-a2A"
   },
   "source": [
    "## Problem 4 [1p]\n",
    "Replace the first nonlinearity with the [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation function. Find a network architecture which reliably learns the 3D XOR problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGgtpe-w-asB"
   },
   "source": [
    "## Problem 5 [1p]\n",
    "Add a second hidden layer to your network, implement the forward and backward pass, then demonstrate training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe-pcFeO-aiE"
   },
   "source": [
    "## Problem 6 [2p]\n",
    "Implement a way to have a _variable number_ of hidden layers. Check how deep digmoid or ReLU networks you  can train. For simplicity you can assume that all hidden layers have the same number of neurons, and use the same activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIpn17Cm-aW7"
   },
   "source": [
    "## Problem 7 [2p]\n",
    "For each weight matrix $w\\in\\mathbb{R}^{n\\times m}$, add a randomly initialized `backward weight` $w_b\\in\\mathbb{R}^{m\\times n}$, which will not change during training. Change the backward pass to use $w_b$ instead of $w^T$, getting an approxmatoin of the true gradient. Can you get your network to train?\n",
    "\n",
    "NB: this approach, dubbed [feedback alignment](https://www.nature.com/articles/ncomms13276), was proposed to make error backpropagation more biologically plausible, by providing a solution to the \"weight transport problem\". Regular backpropagation requires that neurons not only know their incoming weights (thet they control), but also their outgoing weights (that are controlled by neurons in the upper layers). This is nearly impossible in a real brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXJaoHSH0DZO"
   },
   "source": [
    "# Solutions and starter code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "YiTEWD2oqW0Y"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqtfJKR40J3x"
   },
   "source": [
    "XOR dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "lYEbCfbSpv5M",
    "outputId": "48a99aad-e15b-4c7b-f881-bbfbe1941a15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.05, 1.05, -0.05, 1.05)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's define a XOR dataset\n",
    "\n",
    "# X will be matrix of N 2-dimensional inputs\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1],], dtype=np.float32)\n",
    "# Y is a matrix of N numners - answers\n",
    "Y = np.array([[0], [1], [1], [0],], dtype=np.float32)\n",
    "\n",
    "plt.scatter(\n",
    "    X[:, 0], X[:, 1], c=Y[:, 0],\n",
    ")\n",
    "plt.xlabel(\"X[0]\")\n",
    "plt.ylabel(\"X[1]\")\n",
    "plt.axis(\"square\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb3azMn929_I"
   },
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZCM_hdELE04"
   },
   "source": [
    "The code below contains a mock-up of a two-layer neural network. Fill in the code and manually set weights to solve the XOR problem.\n",
    "\n",
    "Please note: the shapes are set to be compatible with PyTorch's conventions:\n",
    "* a batch containing $N$ $D$-dimensional examples has shape $N\\times D$ (each example is a row!)\n",
    "* a weight matrix in a linear layer with $I$ inputs and $O$ outputs has shape $O \\times I$\n",
    "* a bias vector is a 1D vector. Please note that [broadcasting rules](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) allow us to think about it as a $1 \\times D$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "lrrRuk6zLiF0"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "class SmallNet:\n",
    "    def __init__(self, in_features, num_hidden, dtype=np.float32):\n",
    "        self.W1 = np.zeros((num_hidden, in_features), dtype=dtype)\n",
    "        self.b1 = np.zeros((num_hidden,), dtype=dtype)\n",
    "        self.W2 = np.zeros((1, num_hidden), dtype=dtype)\n",
    "        self.b2 = np.zeros((1,), dtype=dtype)\n",
    "        self.num_hidden=num_hidden\n",
    "        self.in_features=in_features\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        # TODO for Problem 2:\n",
    "        # set all parameters to small random values, e.g. from N(0, 0.5)\n",
    "        odchStd=np.sqrt(0.5)\n",
    "        self.W1=np.random.randn(self.num_hidden, self.in_features)/odchStd\n",
    "        self.b1=np.random.randn(self.num_hidden)/odchStd\n",
    "        self.W2=np.random.randn(1,self.num_hidden)/odchStd\n",
    "        self.b2=np.random.randn(1)/odchStd\n",
    "\n",
    "    def forward(self, X, Y=None, do_backward=False):\n",
    "        # TODO Problem 1: Fill in details of forward propagation\n",
    "\n",
    "        # Input to neurons in 1st layer\n",
    "        A1 = X @ self.W1.T + self.b1               #(N, num_hidden)\n",
    "        # wyjściem jest N x num_hidden, \n",
    "        # a więc pobudzenie każdego neurona z danej warstwy przez kolejne próbki\n",
    "        # Outputs after the sigmoid non-linearity\n",
    "        O1 = sigmoid(A1)                           #(N, num_hidden)\n",
    "        # Inputs to neuron in the second layer\n",
    "        A2 = O1 @ self.W2.T + self.b2              #(N, 1)\n",
    "        # Outputs after the sigmoid non-linearity\n",
    "        O2 = sigmoid(A2)                           #(N, 1)\n",
    "\n",
    "        # When Y is none, simply return the predictions. Else compute the loss\n",
    "        if Y is not None:\n",
    "            loss = -Y * np.log(O2) - (1 - Y) * np.log(1.0 - O2)\n",
    "            # normalize loss by batch size\n",
    "            loss = loss.sum() / X.shape[0]\n",
    "        else:\n",
    "            loss = np.nan\n",
    "\n",
    "        if do_backward:\n",
    "            # TODO in Problem 2:\n",
    "            # fill in the gradient computation\n",
    "            # Please note, thate there is a correspondance between\n",
    "            # the forward and backward pass: with backward computations happening\n",
    "            # in reverse order.\n",
    "            # We save the gradients with respect to parameters as fields of self.\n",
    "            # It is not very elegant, but simplifies training code later on.\n",
    "\n",
    "            # A2_grad is the gradient of loss with respect to A2\n",
    "            # Hint: there is a concise formula for the gradient\n",
    "            # of logistic sigmoid and cross-entropy loss\n",
    "            N=A2.shape[0]\n",
    "            A2_grad = O2-Y #O2 = sigmoid(A2)     # (1, 1)\n",
    "            self.b2_grad = A2_grad.sum(0)/N\n",
    "            self.W2_grad = A2_grad.T @ O1 / N    #(1, num_hidden)\n",
    "            O1_grad = A2_grad @ self.W2 #(N, num_hid)\n",
    "            A1_grad = O1_grad * O1*(1-O1)\n",
    "            self.b1_grad = A1_grad.mean(0)\n",
    "            self.W1_grad = A1_grad.T @ X /N\n",
    "        return O2, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJswvBk0oiIY",
    "outputId": "e6559317-7afa-4509-fbac-4880e73b91cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XORnet([0. 0.]) = 1.928749847963918e-22\n",
      "XORnet([0. 1.]) = 1.0\n",
      "XORnet([1. 0.]) = 1.0\n",
      "XORnet([1. 1.]) = 7.175095973164411e-66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-e8a7f3ae897e>:40: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -Y * np.log(O2) - (1 - Y) * np.log(1.0 - O2)\n",
      "<ipython-input-32-e8a7f3ae897e>:40: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -Y * np.log(O2) - (1 - Y) * np.log(1.0 - O2)\n"
     ]
    }
   ],
   "source": [
    "# TODO Problem 1:\n",
    "# Set by hand the weight values to solve the XOR problem\n",
    "\n",
    "net = SmallNet(2, 2, dtype=np.float64)\n",
    "net.W1 = np.array([[100,100],[100,100]], dtype=np.float64)\n",
    "net.b1 = np.array([-150,-50], dtype=np.float64)\n",
    "net.W2 = np.array([[-200,100]], dtype=np.float64)\n",
    "net.b2 = np.array([-50], dtype=np.float64)\n",
    "\n",
    "# Hint: since we use the logistic sigmoid activation, the weights may need to\n",
    "# be fairly large\n",
    "\n",
    "\n",
    "predictions, loss = net.forward(X, Y, do_backward=False)\n",
    "for x, p in zip(X, predictions):\n",
    "    print(f\"XORnet({x}) = {p[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmxCi5Vl6_xB"
   },
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "eSM5hgJ1mrhY"
   },
   "outputs": [],
   "source": [
    "def check_grad(net, param_name, X, Y, eps=1e-5):\n",
    "    \"\"\"A gradient checking routine\"\"\"\n",
    "\n",
    "    param = getattr(net, param_name)\n",
    "    param_flat_accessor = param.reshape(-1)\n",
    "\n",
    "    grad = np.empty_like(param)\n",
    "    grad_flat_accessor = grad.reshape(-1)\n",
    "\n",
    "    net.forward(X, Y, do_backward=True)\n",
    "    orig_grad = getattr(net, param_name + \"_grad\")\n",
    "    assert param.shape == orig_grad.shape\n",
    "\n",
    "    for i in range(param_flat_accessor.shape[0]):\n",
    "        orig_val = param_flat_accessor[i]\n",
    "        param_flat_accessor[i] = orig_val + eps\n",
    "        _, loss_positive = net.forward(X, Y)\n",
    "        param_flat_accessor[i] = orig_val - eps\n",
    "        _, loss_negative = net.forward(X, Y)\n",
    "        param_flat_accessor[i] = orig_val\n",
    "        grad_flat_accessor[i] = (loss_positive - loss_negative) / (2 * eps)\n",
    "    print(grad)\n",
    "    print(orig_grad)\n",
    "    assert np.allclose(grad, orig_grad)\n",
    "    return grad, orig_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "TTZu0jFEvgXF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n",
      "b1\n",
      "W2\n",
      "b2\n"
     ]
    }
   ],
   "source": [
    "# Hint: use float64 for checking the correctness of the gradient\n",
    "net = SmallNet(2, 2, dtype=np.float64)\n",
    "\n",
    "for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
    "    print(param_name)\n",
    "    check_grad(net, param_name, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mUOs3cVvjM2"
   },
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nn2AAoZo0vjU",
    "outputId": "7b6b5a9a-dea5-4357-e8c1-36a34c7c272a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 steps \tloss=0.7080521136022039\n",
      "after 5000 steps \tloss=0.0009024650794515715\n",
      "after 10000 steps \tloss=0.0004254378007708292\n",
      "after 15000 steps \tloss=0.00027607065552783384\n",
      "after 20000 steps \tloss=0.0002036295803810344\n",
      "after 25000 steps \tloss=0.00016100090204573507\n",
      "after 30000 steps \tloss=0.00013297494631230326\n",
      "after 35000 steps \tloss=0.00011316950500424894\n",
      "after 40000 steps \tloss=9.844261569770985e-05\n",
      "after 45000 steps \tloss=8.706979017494814e-05\n",
      "after 50000 steps \tloss=7.80265012264411e-05\n",
      "after 55000 steps \tloss=7.066614715251925e-05\n",
      "after 60000 steps \tloss=6.456070121227793e-05\n",
      "after 65000 steps \tloss=5.941565784739715e-05\n",
      "after 70000 steps \tloss=5.502179679956744e-05\n",
      "after 75000 steps \tloss=5.1226432514071176e-05\n",
      "after 80000 steps \tloss=4.7915550111983125e-05\n",
      "after 85000 steps \tloss=4.500230687329693e-05\n",
      "after 90000 steps \tloss=4.2419402851165866e-05\n",
      "after 95000 steps \tloss=4.0113883265275306e-05\n",
      "after 100000 steps \tloss=3.804351454454452e-05\n"
     ]
    }
   ],
   "source": [
    "net = SmallNet(2, 10, dtype=np.float64)\n",
    "\n",
    "alpha = 1  # set a learning rate\n",
    "\n",
    "for i in range(100001):\n",
    "    _, loss = net.forward(X, Y, do_backward=True)\n",
    "    if (i % 5000) == 0:\n",
    "        print(f\"after {i} steps \\tloss={loss}\")\n",
    "    for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
    "        param = getattr(net, param_name)\n",
    "        # Hint: use the construct `param[:]` to change the contents of the array!\n",
    "        # Doing instead `param = new_val` simply changes to what the variable\n",
    "        # param points to, without affecting the network!\n",
    "        # alternatively, you could do setattr(net, param_name, new_value)\n",
    "        param[:] = param - alpha * getattr(net, param_name + \"_grad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwpEjpkU1JvK",
    "outputId": "dc044de9-81c1-4944-d9a2-5dcc72bf9a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XORnet([0. 0.]) = 1.0612550837749812e-05\n",
      "XORnet([0. 1.]) = 0.9999600859643327\n",
      "XORnet([1. 0.]) = 0.9999584684862729\n",
      "XORnet([1. 1.]) = 6.0110864088615105e-05\n"
     ]
    }
   ],
   "source": [
    "predictions, loss = net.forward(X, Y, do_backward=True)\n",
    "for x, p in zip(X, predictions):\n",
    "    print(f\"XORnet({x}) = {p[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "U0ZMyHqz8xrC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 steps \tloss=1.5718874513693244\n",
      "after 20000 steps \tloss=0.2818184592687003\n",
      "after 40000 steps \tloss=0.28147172920812413\n",
      "after 60000 steps \tloss=0.28135922361580373\n",
      "after 80000 steps \tloss=0.28129687810937076\n",
      "after 100000 steps \tloss=0.28118874248478076\n",
      "For 2 neurons in hidden layer we got 0.28118874248478076 loss result in 11.844173192977905 seconds\n",
      "after 0 steps \tloss=1.5206986564063634\n",
      "after 20000 steps \tloss=0.0006560006821578997\n",
      "after 40000 steps \tloss=0.0003247787308475312\n",
      "after 60000 steps \tloss=0.0002159153138336793\n",
      "after 80000 steps \tloss=0.00016174456414663112\n",
      "after 100000 steps \tloss=0.00012931838619362847\n",
      "For 3 neurons in hidden layer we got 0.00012931838619362847 loss result in 11.914358377456665 seconds\n",
      "after 0 steps \tloss=1.1954056548609873\n",
      "after 20000 steps \tloss=0.000618195179730488\n",
      "after 40000 steps \tloss=0.0002975091694516104\n",
      "after 60000 steps \tloss=0.00019517097763605522\n",
      "after 80000 steps \tloss=0.0001450792652075392\n",
      "after 100000 steps \tloss=0.00011541726930422609\n",
      "For 5 neurons in hidden layer we got 0.00011541726930422609 loss result in 11.681913614273071 seconds\n",
      "after 0 steps \tloss=1.4818468499122661\n",
      "after 20000 steps \tloss=0.0002688895807971012\n",
      "after 40000 steps \tloss=0.00012256490756731872\n",
      "after 60000 steps \tloss=7.806312755634514e-05\n",
      "after 80000 steps \tloss=5.6857299784533136e-05\n",
      "after 100000 steps \tloss=4.4533449875608786e-05\n",
      "For 10 neurons in hidden layer we got 4.4533449875608786e-05 loss result in 11.786029815673828 seconds\n",
      "after 0 steps \tloss=0.8740632971406223\n",
      "after 20000 steps \tloss=0.00017259068173049803\n",
      "after 40000 steps \tloss=8.068060070775483e-05\n",
      "after 60000 steps \tloss=5.1967827249439495e-05\n",
      "after 80000 steps \tloss=3.810301780549009e-05\n",
      "after 100000 steps \tloss=2.9978004397275773e-05\n",
      "For 20 neurons in hidden layer we got 2.9978004397275773e-05 loss result in 12.79794716835022 seconds\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Generate data for a 3D XOR task\n",
    "# Then estimate the sucess rate of training the network with diferent\n",
    "# hidden sizes.\n",
    "\n",
    "X3 = np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,0,1],[1,1,0],[1,1,1]], dtype=np.float64)\n",
    "Y3 = np.array([0,1,1,0,1,0,0,1], dtype=np.float64).reshape((-1,1))\n",
    "\n",
    "alpha=1\n",
    "\n",
    "for hidden_dim in [2, 3, 5, 10, 20]:\n",
    "    net = SmallNet(3, hidden_dim, dtype=np.float64)\n",
    "    predictions, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    t=time.time()\n",
    "    for i in range(100001):\n",
    "        _, loss = net.forward(X3, Y3, do_backward=True)\n",
    "        if (i % 20000) == 0:\n",
    "            print(f\"after {i} steps \\tloss={loss}\")\n",
    "        for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
    "            param = getattr(net, param_name)\n",
    "            param[:] = param - alpha * getattr(net, param_name + \"_grad\")\n",
    "    print(f\"For {hidden_dim} neurons in hidden layer we got {loss} loss result in {time.time()-t} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuaLEoV-9DLG"
   },
   "source": [
    "## Problem 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "lrrRuk6zLiF0"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "\n",
    "class SmallNetReLu:\n",
    "    def __init__(self, in_features, num_hidden, dtype=np.float32):\n",
    "        self.W1 = np.zeros((num_hidden, in_features), dtype=dtype)\n",
    "        self.b1 = np.zeros((num_hidden,), dtype=dtype)\n",
    "        self.W2 = np.zeros((1, num_hidden), dtype=dtype)\n",
    "        self.b2 = np.zeros((1,), dtype=dtype)\n",
    "        self.num_hidden=num_hidden\n",
    "        self.in_features=in_features\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        # TODO for Problem 2:\n",
    "        # set all parameters to small random values, e.g. from N(0, 0.5)\n",
    "        odchStd=np.sqrt(0.5)\n",
    "        self.W1=np.random.randn(self.num_hidden, self.in_features)/odchStd\n",
    "        self.b1=np.random.randn(self.num_hidden)/odchStd\n",
    "        self.W2=np.random.randn(1,self.num_hidden)/odchStd\n",
    "        self.b2=np.random.randn(1)/odchStd\n",
    "\n",
    "    def forward(self, X, Y=None, do_backward=False):\n",
    "        # TODO Problem 1: Fill in details of forward propagation\n",
    "\n",
    "        # Input to neurons in 1st layer\n",
    "        A1 = X @ self.W1.T + self.b1               #(N, num_hidden)\n",
    "        # wyjściem jest N x num_hidden, \n",
    "        # a więc pobudzenie każdego neurona z danej warstwy przez kolejne próbki\n",
    "        # Outputs after the sigmoid non-linearity\n",
    "        O1 = relu(A1)                           #(N, num_hidden)\n",
    "        # Inputs to neuron in the second layer\n",
    "        A2 = O1 @ self.W2.T + self.b2              #(N, 1)\n",
    "        # Outputs after the sigmoid non-linearity\n",
    "        O2 = sigmoid(A2)                           #(N, 1)\n",
    "\n",
    "        # When Y is none, simply return the predictions. Else compute the loss\n",
    "        if Y is not None:\n",
    "            loss = -Y * np.log(O2) - (1 - Y) * np.log(1.0 - O2)\n",
    "            # normalize loss by batch size\n",
    "            loss = loss.sum() / X.shape[0]\n",
    "        else:\n",
    "            loss = np.nan\n",
    "\n",
    "        if do_backward:\n",
    "            N=A2.shape[0]\n",
    "            A2_grad = O2-Y # (1, 1)\n",
    "            self.b2_grad = A2_grad.sum(0)/N\n",
    "            self.W2_grad = A2_grad.T @ O1 / N    #(1, num_hidden)\n",
    "            O1_grad = A2_grad @ self.W2 #(N, num_hid)\n",
    "            dO1dA1=np.zeros_like(A1)\n",
    "            dO1dA1[O1==A1]=1\n",
    "            A1_grad = O1_grad * dO1dA1\n",
    "            self.b1_grad = A1_grad.mean(0)\n",
    "            self.W1_grad = A1_grad.T @ X /N\n",
    "        return O2, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "TTZu0jFEvgXF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2\n",
      "[[0.         0.23990981]]\n",
      "[[0.         0.23990981]]\n",
      "b2\n",
      "[0.20354752]\n",
      "[0.20354752]\n",
      "b1\n",
      "[0.         0.25273034]\n",
      "[0.         0.25273034]\n",
      "W1\n",
      "[[0.         0.        ]\n",
      " [0.06835365 0.06624276]]\n",
      "[[0.         0.        ]\n",
      " [0.06835365 0.06624276]]\n"
     ]
    }
   ],
   "source": [
    "# Hint: use float64 for checking the correctness of the gradient\n",
    "net = SmallNetReLu(2, 2, dtype=np.float64)\n",
    "\n",
    "for param_name in [\"W2\", \"b2\", \"b1\", \"W1\"]:\n",
    "    print(param_name)\n",
    "    check_grad(net, param_name, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "U0ZMyHqz8xrC",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 steps \tloss=0.7919781273088401\n",
      "after 20000 steps \tloss=0.6931471805599453\n",
      "after 40000 steps \tloss=0.6931471805599453\n",
      "after 60000 steps \tloss=0.6931471805599453\n",
      "after 80000 steps \tloss=0.6931471805599453\n",
      "after 100000 steps \tloss=0.6931471805599453\n",
      "For 2 neurons in hidden layer we got 0.6931471805599453 loss result in 12.550550937652588 seconds\n",
      "after 0 steps \tloss=0.9575184291545026\n",
      "after 20000 steps \tloss=0.5411015807024222\n",
      "after 40000 steps \tloss=0.5411003431839352\n",
      "after 60000 steps \tloss=0.5410987211470366\n",
      "after 80000 steps \tloss=0.5410984916158869\n",
      "after 100000 steps \tloss=0.5410984659742063\n",
      "For 3 neurons in hidden layer we got 0.5410984659742063 loss result in 12.928548812866211 seconds\n",
      "after 0 steps \tloss=0.821576967592114\n",
      "after 20000 steps \tloss=9.33589018011415e-05\n",
      "after 40000 steps \tloss=4.481030597994773e-05\n",
      "after 60000 steps \tloss=2.9252624127604318e-05\n",
      "after 80000 steps \tloss=2.1639086960933636e-05\n",
      "after 100000 steps \tloss=1.713489956598685e-05\n",
      "For 5 neurons in hidden layer we got 1.713489956598685e-05 loss result in 12.828678846359253 seconds\n",
      "after 0 steps \tloss=1.6833408419181586\n",
      "after 20000 steps \tloss=5.5291488824433686e-05\n",
      "after 40000 steps \tloss=2.5023419549142654e-05\n",
      "after 60000 steps \tloss=1.5820076635657812e-05\n",
      "after 80000 steps \tloss=1.13444224637866e-05\n",
      "after 100000 steps \tloss=8.749560578341109e-06\n",
      "For 10 neurons in hidden layer we got 8.749560578341109e-06 loss result in 12.988407611846924 seconds\n",
      "after 0 steps \tloss=7.474998935932788\n",
      "after 20000 steps \tloss=4.110564489042214e-05\n",
      "after 40000 steps \tloss=1.939225779349676e-05\n",
      "after 60000 steps \tloss=1.2462807151799195e-05\n",
      "after 80000 steps \tloss=9.102515494208568e-06\n",
      "after 100000 steps \tloss=7.133066696785291e-06\n",
      "For 20 neurons in hidden layer we got 7.133066696785291e-06 loss result in 13.364641189575195 seconds\n"
     ]
    }
   ],
   "source": [
    "X3 = np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,0,1],[1,1,0],[1,1,1]], dtype=np.float64)\n",
    "Y3 = np.array([0,1,1,0,1,0,0,1], dtype=np.float64).reshape((-1,1))\n",
    "\n",
    "alpha=1\n",
    "\n",
    "for hidden_dim in [2, 3, 5, 10, 20]:\n",
    "    net = SmallNetReLu(3, hidden_dim, dtype=np.float64)\n",
    "    predictions, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    t=time.time()\n",
    "    for i in range(100001):\n",
    "        _, loss = net.forward(X3, Y3, do_backward=True)\n",
    "        if (i % 20000) == 0:\n",
    "            print(f\"after {i} steps \\tloss={loss}\")\n",
    "        for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
    "            param = getattr(net, param_name)\n",
    "            param[:] = param - alpha * getattr(net, param_name + \"_grad\")\n",
    "    print(f\"For {hidden_dim} neurons in hidden layer we got {loss} loss result in {time.time()-t} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PcNxrCt-NcN"
   },
   "source": [
    "## Problem 6 (i 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "6Brepirl-Nln"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "class VaNet:\n",
    "    def __init__(self, in_features, num_hidden, num_hidden_lay, dtype=np.float32, ifSig=True):\n",
    "        if num_hidden_lay <=1:\n",
    "            raise RuntimeError(\"Za malo ukrytych warstw\")\n",
    "        self.num_hidden=num_hidden\n",
    "        self.in_features=in_features\n",
    "        self.num_hidden_lay=num_hidden_lay\n",
    "        self.ifSig=ifSig\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        odchStd=np.sqrt(0.5)\n",
    "        self.W=[np.random.randn(self.num_hidden, self.in_features)/odchStd]\n",
    "        self.b=[np.random.randn(self.num_hidden)/odchStd]\n",
    "        for i in range(self.num_hidden_lay-1):\n",
    "            self.W.append(np.random.randn(self.num_hidden, self.num_hidden)/odchStd)\n",
    "            self.b.append(np.random.randn(self.num_hidden)/odchStd)\n",
    "        self.W.append(np.random.randn(1,self.num_hidden)/odchStd)\n",
    "        self.b.append(np.random.randn(1)/odchStd)\n",
    "\n",
    "    def forward(self, X, Y=None, do_backward=False):\n",
    "        A = [X @ self.W[0].T + self.b[0]]\n",
    "        if self.ifSig:\n",
    "            O = [sigmoid(A[0])]\n",
    "        else:\n",
    "            O= [relu(A[0])]\n",
    "        for i in range(1,len(self.W)-1):\n",
    "            W=self.W[i]\n",
    "            b=self.b[i]\n",
    "            A.append(O[-1]@W.T+b)\n",
    "            if self.ifSig:\n",
    "                O.append(sigmoid(A[-1]))\n",
    "            else:\n",
    "                O.append(relu(A[-1]))\n",
    "        A.append(O[-1]@self.W[-1].T+self.b[-1])\n",
    "        O.append(sigmoid(A[-1]))\n",
    "\n",
    "        # When Y is none, simply return the predictions. Else compute the loss\n",
    "        if Y is not None:\n",
    "            loss = -Y * np.log(O[-1]) - (1 - Y) * np.log(1.0 - O[-1])\n",
    "            # normalize loss by batch size\n",
    "            loss = loss.sum() / X.shape[0]\n",
    "        else:\n",
    "            loss = np.nan\n",
    "\n",
    "        if do_backward:\n",
    "            N=A[-1].shape[0]\n",
    "            A_grad = O[-1]-Y\n",
    "            self.b_grad = [A_grad.sum(0)/N]\n",
    "            self.W_grad = [A_grad.T @ O[-2] / N]\n",
    "            for i in range(len(self.W)-1,1,-1):\n",
    "                O_grad = A_grad @ self.W[i]\n",
    "                if self.ifSig:\n",
    "                    A_grad = O_grad * O[i-1]*(1-O[i-1])\n",
    "                else:\n",
    "                    dOdA=np.zeros_like(O_grad)\n",
    "                    dOdA[O[i-1]==A[i-1]]=1\n",
    "                    A_grad = O_grad * dOdA\n",
    "                self.b_grad.append(A_grad.mean(0))\n",
    "                self.W_grad.append(A_grad.T @ O[i-2] /N)\n",
    "            O_grad = A_grad @ self.W[1]\n",
    "            if self.ifSig:\n",
    "                A_grad = O_grad * O[0]*(1-O[0])\n",
    "            else:\n",
    "                dOdA=np.zeros_like(O_grad)\n",
    "                dOdA[O[0]==A[0]]=1\n",
    "                A_grad = O_grad * dOdA\n",
    "            self.b_grad.append(A_grad.mean(0))\n",
    "            self.W_grad.append(A_grad.T @ X /N)\n",
    "            \n",
    "            self.b_grad.reverse()\n",
    "            self.W_grad.reverse()\n",
    "        return O[-1], loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grad_va(net, param_name, nr, X, Y, eps=1e-5):\n",
    "    \"\"\"A gradient checking routine\"\"\"\n",
    "\n",
    "    param = getattr(net, param_name)[nr]\n",
    "    param_flat_accessor = param.reshape(-1)\n",
    "\n",
    "    grad = np.empty_like(param)\n",
    "    grad_flat_accessor = grad.reshape(-1)\n",
    "\n",
    "    net.forward(X, Y, do_backward=True)\n",
    "    orig_grad = getattr(net, param_name + \"_grad\")[nr]\n",
    "    assert param.shape == orig_grad.shape\n",
    "\n",
    "    for i in range(param_flat_accessor.shape[0]):\n",
    "        orig_val = param_flat_accessor[i]\n",
    "        param_flat_accessor[i] = orig_val + eps\n",
    "        _, loss_positive = net.forward(X, Y)\n",
    "        param_flat_accessor[i] = orig_val - eps\n",
    "        _, loss_negative = net.forward(X, Y)\n",
    "        param_flat_accessor[i] = orig_val\n",
    "        grad_flat_accessor[i] = (loss_positive - loss_negative) / (2 * eps)\n",
    "    #print(grad)\n",
    "    #print(orig_grad)\n",
    "    assert np.allclose(grad, orig_grad)\n",
    "    return grad, orig_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "TTZu0jFEvgXF",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = VaNet(2, 2, 2,dtype=np.float64)\n",
    "for i in range(3):\n",
    "    for param_name in [\"W\", \"b\"]:\n",
    "        #print(param_name, i)\n",
    "        check_grad_va(net, param_name, i, X, Y)\n",
    "\n",
    "net = VaNet(2, 2, 2,dtype=np.float64, ifSig=False)\n",
    "for i in range(3):\n",
    "    for param_name in [\"W\", \"b\"]:\n",
    "        #print(param_name, i)\n",
    "        check_grad_va(net, param_name, i, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "U0ZMyHqz8xrC",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "after 0 steps \tloss=1.0354566571627126\n",
      "after 20000 steps \tloss=0.5202425331096188\n",
      "after 40000 steps \tloss=0.5200319540854809\n",
      "after 60000 steps \tloss=0.5199677374067148\n",
      "after 80000 steps \tloss=0.5199370512018205\n",
      "after 100000 steps \tloss=0.5199192847120411\n",
      "For 2 neurons in hidden layer and 2 layers we got 0.5199192847120411 loss result in 17.86483669281006 seconds\n",
      "after 0 steps \tloss=0.7021782528938354\n",
      "after 20000 steps \tloss=0.0006867200732114645\n",
      "after 40000 steps \tloss=0.00033211782302871066\n",
      "after 60000 steps \tloss=0.00021903854749362825\n",
      "after 80000 steps \tloss=0.00016341394155691818\n",
      "after 100000 steps \tloss=0.00013032556888952564\n",
      "For 3 neurons in hidden layer and 2 layers we got 0.00013032556888952564 loss result in 18.164801120758057 seconds\n",
      "after 0 steps \tloss=4.493490652651193\n",
      "after 20000 steps \tloss=7.927463598900046e-05\n",
      "after 40000 steps \tloss=3.724649829456868e-05\n",
      "after 60000 steps \tloss=2.4176564760679e-05\n",
      "after 80000 steps \tloss=1.7849878238272607e-05\n",
      "after 100000 steps \tloss=1.4128448060773841e-05\n",
      "For 5 neurons in hidden layer and 2 layers we got 1.4128448060773841e-05 loss result in 18.43673801422119 seconds\n",
      "after 0 steps \tloss=0.737182464747127\n",
      "after 20000 steps \tloss=7.177609807811103e-05\n",
      "after 40000 steps \tloss=3.1722322028750534e-05\n",
      "after 60000 steps \tloss=1.9838951125990073e-05\n",
      "after 80000 steps \tloss=1.4256716561234884e-05\n",
      "after 100000 steps \tloss=1.104592956735935e-05\n",
      "For 10 neurons in hidden layer and 2 layers we got 1.104592956735935e-05 loss result in 18.949488401412964 seconds\n",
      "after 0 steps \tloss=1.093972826820925\n",
      "after 20000 steps \tloss=0.00013627629895225256\n",
      "after 40000 steps \tloss=6.348696925302204e-05\n",
      "after 60000 steps \tloss=4.083305304260208e-05\n",
      "after 80000 steps \tloss=2.9917730857342994e-05\n",
      "after 100000 steps \tloss=2.353009446226984e-05\n",
      "For 20 neurons in hidden layer and 2 layers we got 2.353009446226984e-05 loss result in 20.89621925354004 seconds\n",
      "================================\n",
      "after 0 steps \tloss=1.3748106069999417\n",
      "after 20000 steps \tloss=0.0003453721838846567\n",
      "after 40000 steps \tloss=0.00010266234049113312\n",
      "after 60000 steps \tloss=6.043622261065902e-05\n",
      "after 80000 steps \tloss=4.2854245089718e-05\n",
      "after 100000 steps \tloss=3.320827184907916e-05\n",
      "For 2 neurons in hidden layer and 4 layers we got 3.320827184907916e-05 loss result in 29.73110294342041 seconds\n",
      "after 0 steps \tloss=0.7982859877785352\n",
      "after 20000 steps \tloss=0.00010627736401736281\n",
      "after 40000 steps \tloss=5.061621831108185e-05\n",
      "after 60000 steps \tloss=3.32091161501279e-05\n",
      "after 80000 steps \tloss=2.4704412784567137e-05\n",
      "after 100000 steps \tloss=1.9663447622982996e-05\n",
      "For 3 neurons in hidden layer and 4 layers we got 1.9663447622982996e-05 loss result in 29.8267662525177 seconds\n",
      "after 0 steps \tloss=0.7417147273657068\n",
      "after 20000 steps \tloss=9.87799456716433e-05\n",
      "after 40000 steps \tloss=4.1625342237819236e-05\n",
      "after 60000 steps \tloss=2.5674293673233696e-05\n",
      "after 80000 steps \tloss=1.8357860648519578e-05\n",
      "after 100000 steps \tloss=1.4203764471372588e-05\n",
      "For 5 neurons in hidden layer and 4 layers we got 1.4203764471372588e-05 loss result in 29.228864669799805 seconds\n",
      "after 0 steps \tloss=1.2295266415617323\n",
      "after 20000 steps \tloss=4.60041418519582e-05\n",
      "after 40000 steps \tloss=2.1917722181421117e-05\n",
      "after 60000 steps \tloss=1.4253494493370834e-05\n",
      "after 80000 steps \tloss=1.05121014040896e-05\n",
      "after 100000 steps \tloss=8.303011397510775e-06\n",
      "For 10 neurons in hidden layer and 4 layers we got 8.303011397510775e-06 loss result in 30.985073804855347 seconds\n",
      "after 0 steps \tloss=1.1752779286732271\n",
      "after 20000 steps \tloss=4.964119214223056e-05\n",
      "after 40000 steps \tloss=2.3708793420706303e-05\n",
      "after 60000 steps \tloss=1.5435548336774584e-05\n",
      "after 80000 steps \tloss=1.1397900137900421e-05\n",
      "after 100000 steps \tloss=9.015417155225667e-06\n",
      "For 20 neurons in hidden layer and 4 layers we got 9.015417155225667e-06 loss result in 34.82991051673889 seconds\n",
      "================================\n",
      "after 0 steps \tloss=0.9747685218583615\n",
      "after 20000 steps \tloss=0.6931474365745391\n",
      "after 40000 steps \tloss=0.6931473884393186\n",
      "after 60000 steps \tloss=0.6931473507550656\n",
      "after 80000 steps \tloss=0.6931473205904131\n",
      "after 100000 steps \tloss=0.6931472959809601\n",
      "For 2 neurons in hidden layer and 6 layers we got 0.6931472959809601 loss result in 40.31498408317566 seconds\n",
      "after 0 steps \tloss=0.7016965202842055\n",
      "after 20000 steps \tloss=0.5201798551433361\n",
      "after 40000 steps \tloss=0.51997156601583\n",
      "after 60000 steps \tloss=0.5198418801024144\n",
      "after 80000 steps \tloss=0.28122240851930047\n",
      "after 100000 steps \tloss=0.2811903086601703\n",
      "For 3 neurons in hidden layer and 6 layers we got 0.2811903086601703 loss result in 39.1919641494751 seconds\n",
      "after 0 steps \tloss=1.1200294334335559\n",
      "after 20000 steps \tloss=9.932094513096003e-05\n",
      "after 40000 steps \tloss=4.81712254783384e-05\n",
      "after 60000 steps \tloss=3.1647362451237145e-05\n",
      "after 80000 steps \tloss=2.3478246528281175e-05\n",
      "after 100000 steps \tloss=1.860609450933982e-05\n",
      "For 5 neurons in hidden layer and 6 layers we got 1.860609450933982e-05 loss result in 39.02533841133118 seconds\n",
      "after 0 steps \tloss=0.7044909518207769\n",
      "after 20000 steps \tloss=3.298313687358711e-05\n",
      "after 40000 steps \tloss=1.598046516407268e-05\n",
      "after 60000 steps \tloss=1.050214007037997e-05\n",
      "after 80000 steps \tloss=7.807079293960581e-06\n",
      "after 100000 steps \tloss=6.206619738925735e-06\n",
      "For 10 neurons in hidden layer and 6 layers we got 6.206619738925735e-06 loss result in 43.06627607345581 seconds\n",
      "after 0 steps \tloss=2.5807385108571115\n",
      "after 20000 steps \tloss=1.9312192466458482e-05\n",
      "after 40000 steps \tloss=9.421859728906403e-06\n",
      "after 60000 steps \tloss=6.209274888249019e-06\n",
      "after 80000 steps \tloss=4.623039921474575e-06\n",
      "after 100000 steps \tloss=3.678971019755887e-06\n",
      "For 20 neurons in hidden layer and 6 layers we got 3.678971019755887e-06 loss result in 48.71979808807373 seconds\n",
      "================================\n",
      "after 0 steps \tloss=0.9349467264968061\n",
      "after 20000 steps \tloss=0.6931471805599453\n",
      "after 40000 steps \tloss=0.6931471805599453\n",
      "after 60000 steps \tloss=0.6931471805599453\n",
      "after 80000 steps \tloss=0.6931471805599453\n",
      "after 100000 steps \tloss=0.6931471805599453\n",
      "For 2 neurons in hidden layer and 2 layers we got 0.6931471805599453 loss result in 19.67262840270996 seconds\n",
      "after 0 steps \tloss=0.9250132270263851\n",
      "after 20000 steps \tloss=0.6931471805599453\n",
      "after 40000 steps \tloss=0.6931471805599453\n",
      "after 60000 steps \tloss=0.6931471805599453\n",
      "after 80000 steps \tloss=0.6931471805599453\n",
      "after 100000 steps \tloss=0.6931471805599453\n",
      "For 3 neurons in hidden layer and 2 layers we got 0.6931471805599453 loss result in 20.534488677978516 seconds\n",
      "after 0 steps \tloss=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-246-18b4b4428a58>:45: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -Y * np.log(O[-1]) - (1 - Y) * np.log(1.0 - O[-1])\n",
      "<ipython-input-246-18b4b4428a58>:45: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -Y * np.log(O[-1]) - (1 - Y) * np.log(1.0 - O[-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 20000 steps \tloss=0.6931471805599453\n",
      "after 40000 steps \tloss=0.6931471805599453\n",
      "after 60000 steps \tloss=0.6931471805599453\n",
      "after 80000 steps \tloss=0.6931471805599453\n",
      "after 100000 steps \tloss=0.6931471805599453\n",
      "For 5 neurons in hidden layer and 2 layers we got 0.6931471805599453 loss result in 20.007113218307495 seconds\n",
      "after 0 steps \tloss=nan\n",
      "after 20000 steps \tloss=4.400471911297245e-05\n",
      "after 40000 steps \tloss=2.2658464834619484e-05\n",
      "after 60000 steps \tloss=1.6022985629444472e-05\n",
      "after 80000 steps \tloss=1.241406221414754e-05\n",
      "after 100000 steps \tloss=1.0137161389613599e-05\n",
      "For 10 neurons in hidden layer and 2 layers we got 1.0137161389613599e-05 loss result in 19.905860662460327 seconds\n",
      "after 0 steps \tloss=8.487952145759046\n",
      "after 20000 steps \tloss=0.6931471805599453\n",
      "after 40000 steps \tloss=0.6931471805599453\n",
      "after 60000 steps \tloss=0.6931471805599453\n",
      "after 80000 steps \tloss=0.6931471805599453\n",
      "after 100000 steps \tloss=0.6931471805599453\n",
      "For 20 neurons in hidden layer and 2 layers we got 0.6931471805599453 loss result in 21.179141521453857 seconds\n",
      "================================\n",
      "after 0 steps \tloss=1.108229305263631\n",
      "after 20000 steps \tloss=0.5623351446188083\n",
      "after 40000 steps \tloss=0.5623351446188083\n",
      "after 60000 steps \tloss=0.5623351446188083\n",
      "after 80000 steps \tloss=0.5623351446188083\n",
      "after 100000 steps \tloss=0.5623351446188083\n",
      "For 2 neurons in hidden layer and 4 layers we got 0.5623351446188083 loss result in 31.480802297592163 seconds\n",
      "after 0 steps \tloss=7.705269272358123\n",
      "after 20000 steps \tloss=0.6931471805599453\n",
      "after 40000 steps \tloss=0.6931471805599453\n",
      "after 60000 steps \tloss=0.6931471805599453\n",
      "after 80000 steps \tloss=0.6931471805599453\n",
      "after 100000 steps \tloss=0.6931471805599453\n",
      "For 3 neurons in hidden layer and 4 layers we got 0.6931471805599453 loss result in 31.471301555633545 seconds\n",
      "after 0 steps \tloss=1.047036943400946\n",
      "after 20000 steps \tloss=0.6931471805599453\n",
      "after 40000 steps \tloss=0.6931471805599453\n",
      "after 60000 steps \tloss=0.6931471805599453\n",
      "after 80000 steps \tloss=0.6931471805599453\n",
      "after 100000 steps \tloss=0.6931471805599453\n",
      "For 5 neurons in hidden layer and 4 layers we got 0.6931471805599453 loss result in 32.475581884384155 seconds\n",
      "after 0 steps \tloss=10.051233682664769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-246-18b4b4428a58>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 20000 steps \tloss=0.6931471805599453\n",
      "after 40000 steps \tloss=0.6931471805599453\n",
      "after 60000 steps \tloss=0.6931471805599453\n",
      "after 80000 steps \tloss=0.6931471805599453\n",
      "after 100000 steps \tloss=0.6931471805599453\n",
      "For 10 neurons in hidden layer and 4 layers we got 0.6931471805599453 loss result in 32.25173497200012 seconds\n",
      "after 0 steps \tloss=nan\n",
      "after 20000 steps \tloss=0.6931471805599453\n",
      "after 40000 steps \tloss=0.6931471805599453\n",
      "after 60000 steps \tloss=0.6931471805599453\n",
      "after 80000 steps \tloss=0.6931471805599453\n",
      "after 100000 steps \tloss=0.6931471805599453\n",
      "For 20 neurons in hidden layer and 4 layers we got 0.6931471805599453 loss result in 36.12034320831299 seconds\n",
      "================================\n",
      "after 0 steps \tloss=1.902534550732017\n",
      "after 20000 steps \tloss=0.6931471805599453\n",
      "after 40000 steps \tloss=0.6931471805599453\n",
      "after 60000 steps \tloss=0.6931471805599453\n",
      "after 80000 steps \tloss=0.6931471805599453\n",
      "after 100000 steps \tloss=0.6931471805599453\n",
      "For 2 neurons in hidden layer and 6 layers we got 0.6931471805599453 loss result in 43.44946336746216 seconds\n",
      "after 0 steps \tloss=5.338753975489152\n",
      "after 20000 steps \tloss=0.6931471805599453\n",
      "after 40000 steps \tloss=0.6931471805599453\n",
      "after 60000 steps \tloss=0.6931471805599453\n",
      "after 80000 steps \tloss=0.6931471805599453\n",
      "after 100000 steps \tloss=0.6931471805599453\n",
      "For 3 neurons in hidden layer and 6 layers we got 0.6931471805599453 loss result in 43.655670166015625 seconds\n",
      "after 0 steps \tloss=36.9420482729339\n",
      "after 20000 steps \tloss=0.6931471805599453\n",
      "after 40000 steps \tloss=0.6931471805599453\n",
      "after 60000 steps \tloss=0.6931471805599453\n",
      "after 80000 steps \tloss=0.6931471805599453\n",
      "after 100000 steps \tloss=0.6931471805599453\n",
      "For 5 neurons in hidden layer and 6 layers we got 0.6931471805599453 loss result in 44.224926710128784 seconds\n",
      "after 0 steps \tloss=nan\n",
      "after 20000 steps \tloss=0.6931471805599453\n",
      "after 40000 steps \tloss=0.6931471805599453\n",
      "after 60000 steps \tloss=0.6931471805599453\n",
      "after 80000 steps \tloss=0.6931471805599453\n",
      "after 100000 steps \tloss=0.6931471805599453\n",
      "For 10 neurons in hidden layer and 6 layers we got 0.6931471805599453 loss result in 45.16697859764099 seconds\n",
      "after 0 steps \tloss=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-246-18b4b4428a58>:40: RuntimeWarning: overflow encountered in matmul\n",
      "  A.append(O[-1]@self.W[-1].T+self.b[-1])\n",
      "<ipython-input-246-18b4b4428a58>:57: RuntimeWarning: overflow encountered in matmul\n",
      "  O_grad = A_grad @ self.W[i]\n",
      "<ipython-input-246-18b4b4428a58>:63: RuntimeWarning: invalid value encountered in multiply\n",
      "  A_grad = O_grad * dOdA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 20000 steps \tloss=nan\n",
      "after 40000 steps \tloss=nan\n",
      "after 60000 steps \tloss=nan\n",
      "after 80000 steps \tloss=nan\n",
      "after 100000 steps \tloss=nan\n",
      "For 20 neurons in hidden layer and 6 layers we got nan loss result in 51.043829917907715 seconds\n"
     ]
    }
   ],
   "source": [
    "X3 = np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,0,1],[1,1,0],[1,1,1]], dtype=np.float64)\n",
    "Y3 = np.array([0,1,1,0,1,0,0,1], dtype=np.float64).reshape((-1,1))\n",
    "\n",
    "alpha=1\n",
    "\n",
    "for ifSig in [True, False]:\n",
    "    for lay in [2,4,6]:\n",
    "        print(\"================================\")\n",
    "        for hidden_dim in [2, 3, 5, 10, 20]:\n",
    "            net = VaNet(3, hidden_dim, lay, dtype=np.float64, ifSig=ifSig)\n",
    "            predictions, loss = net.forward(X3, Y3, do_backward=True)\n",
    "            t=time.time()\n",
    "            for i in range(100001):\n",
    "                _, loss = net.forward(X3, Y3, do_backward=True)\n",
    "                if (i % 20000) == 0:\n",
    "                    print(f\"after {i} steps \\tloss={loss}\")\n",
    "                for param_name in [\"W\", \"b\"]:\n",
    "                    for nr in range(lay+1):\n",
    "                        param = getattr(net, param_name)[nr]\n",
    "                        param[:] = param - alpha * getattr(net, param_name + \"_grad\")[nr]\n",
    "            print(f\"For {hidden_dim} neurons in hidden layer and {lay} layers we got {loss} loss result in {time.time()-t} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWuv7Q77-Nut"
   },
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "avuvSoWY-N4Z"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "class FBSmallNet:\n",
    "    def __init__(self, in_features, num_hidden, dtype=np.float32):\n",
    "        self.num_hidden=num_hidden\n",
    "        self.in_features=in_features\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        # TODO for Problem 2:\n",
    "        # set all parameters to small random values, e.g. from N(0, 0.5)\n",
    "        odchStd=np.sqrt(0.5)\n",
    "        self.W1=np.random.randn(self.num_hidden, self.in_features)/odchStd\n",
    "        self.b1=np.random.randn(self.num_hidden)/odchStd\n",
    "        self.W2=np.random.randn(1,self.num_hidden)/odchStd\n",
    "        self.W2T=np.random.randn(1,self.num_hidden)/odchStd\n",
    "        self.b2=np.random.randn(1)/odchStd\n",
    "\n",
    "    def forward(self, X, Y=None, do_backward=False):\n",
    "        # Input to neurons in 1st layer\n",
    "        A1 = X @ self.W1.T + self.b1               #(N, num_hidden)\n",
    "        # Outputs after the sigmoid non-linearity\n",
    "        O1 = sigmoid(A1)                           #(N, num_hidden)\n",
    "        # Inputs to neuron in the second layer\n",
    "        A2 = O1 @ self.W2.T + self.b2              #(N, 1)\n",
    "        # Outputs after the sigmoid non-linearity\n",
    "        O2 = sigmoid(A2)                           #(N, 1)\n",
    "\n",
    "        # When Y is none, simply return the predictions. Else compute the loss\n",
    "        if Y is not None:\n",
    "            loss = -Y * np.log(O2) - (1 - Y) * np.log(1.0 - O2)\n",
    "            # normalize loss by batch size\n",
    "            loss = loss.sum() / X.shape[0]\n",
    "        else:\n",
    "            loss = np.nan\n",
    "\n",
    "        if do_backward:\n",
    "            N=A2.shape[0]\n",
    "            A2_grad = O2-Y #O2 = sigmoid(A2)     # (1, 1)\n",
    "            self.b2_grad = A2_grad.sum(0)/N\n",
    "            self.W2_grad = A2_grad.T @ O1 / N    #(1, num_hidden)\n",
    "            O1_grad = A2_grad @ self.W2T #(N, num_hid)\n",
    "            A1_grad = O1_grad * O1*(1-O1)\n",
    "            self.b1_grad = A1_grad.mean(0)\n",
    "            self.W1_grad = A1_grad.T @ X /N\n",
    "        return O2, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "U0ZMyHqz8xrC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 steps \tloss=2.1826081334168808\n",
      "after 20000 steps \tloss=0.5234199296696409\n",
      "after 40000 steps \tloss=0.5217947641345557\n",
      "after 60000 steps \tloss=0.5212120709156919\n",
      "after 80000 steps \tloss=0.5209076866029075\n",
      "after 100000 steps \tloss=0.5207192242421059\n",
      "For 2 neurons in hidden layer we got 0.5207192242421059 loss result in 11.34013056755066 seconds\n",
      "after 0 steps \tloss=1.7429716109985358\n",
      "after 20000 steps \tloss=0.0008689238661701605\n",
      "after 40000 steps \tloss=0.0004189292114642601\n",
      "after 60000 steps \tloss=0.0002745671098647464\n",
      "after 80000 steps \tloss=0.00020373958574149693\n",
      "after 100000 steps \tloss=0.00016175794531524137\n",
      "For 3 neurons in hidden layer we got 0.00016175794531524137 loss result in 11.386260509490967 seconds\n",
      "after 0 steps \tloss=0.9222479086761678\n",
      "after 20000 steps \tloss=0.0010724079416242832\n",
      "after 40000 steps \tloss=0.0005070799932105597\n",
      "after 60000 steps \tloss=0.0003294585352980529\n",
      "after 80000 steps \tloss=0.00024315898320218052\n",
      "after 100000 steps \tloss=0.0001923193068852436\n",
      "For 5 neurons in hidden layer we got 0.0001923193068852436 loss result in 11.370650053024292 seconds\n",
      "after 0 steps \tloss=0.6983994671195811\n",
      "after 20000 steps \tloss=0.0004506717527267782\n",
      "after 40000 steps \tloss=0.00020380067452379567\n",
      "after 60000 steps \tloss=0.0001298438531325336\n",
      "after 80000 steps \tloss=9.472455690581278e-05\n",
      "after 100000 steps \tloss=7.432774539928191e-05\n",
      "For 10 neurons in hidden layer we got 7.432774539928191e-05 loss result in 11.688219785690308 seconds\n",
      "after 0 steps \tloss=0.9011787382310228\n",
      "after 20000 steps \tloss=0.00036932153618143756\n",
      "after 40000 steps \tloss=0.00017550956114949153\n",
      "after 60000 steps \tloss=0.00011408095666767928\n",
      "after 80000 steps \tloss=8.416610578748391e-05\n",
      "after 100000 steps \tloss=6.652991445089836e-05\n",
      "For 20 neurons in hidden layer we got 6.652991445089836e-05 loss result in 12.433170795440674 seconds\n"
     ]
    }
   ],
   "source": [
    "X3 = np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,0,1],[1,1,0],[1,1,1]], dtype=np.float64)\n",
    "Y3 = np.array([0,1,1,0,1,0,0,1], dtype=np.float64).reshape((-1,1))\n",
    "\n",
    "alpha=1\n",
    "\n",
    "for hidden_dim in [2, 3, 5, 10, 20]:\n",
    "    net = FBSmallNet(3, hidden_dim, dtype=np.float64)\n",
    "    predictions, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    t=time.time()\n",
    "    for i in range(100001):\n",
    "        _, loss = net.forward(X3, Y3, do_backward=True)\n",
    "        if (i % 20000) == 0:\n",
    "            print(f\"after {i} steps \\tloss={loss}\")\n",
    "        for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
    "            param = getattr(net, param_name)\n",
    "            param[:] = param - alpha * getattr(net, param_name + \"_grad\")\n",
    "    print(f\"For {hidden_dim} neurons in hidden layer we got {loss} loss result in {time.time()-t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Assignment1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
