{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janchorowski/dl_uwr/blob/summer2021/Assignments/Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mUxp5svuqQL"
      },
      "source": [
        "# Assignment 4\n",
        "\n",
        "**Submission deadlines:**\n",
        "- get at least 4 points by Tuesday, 11.05.2021\n",
        "- remaining points: last lab session before or on Tuesday, 18.05.2021\n",
        "\n",
        "**Points:** Aim to get 12 out of 15+ possible points\n",
        "\n",
        "\n",
        "## Submission instructions\n",
        "The class is held remotely. To submit your solutions please show the notebook over the video call. Make sure you know all the questions and answers, and that the notebook contains results (before presentation do `Runtime -> Restart and run all`)\n",
        "\n",
        "We provide starter code, however, you are not required to use it as long as you properly solve the tasks.\n",
        "\n",
        "As always, please submit corrections using GitHub's Pull Requests to https://github.com/janchorowski/dl_uwr."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyuBQmWIuqQW"
      },
      "source": [
        "# Problem 1: Word Embeddings [4p]\n",
        "\n",
        "Many natural language processing tasks requrie continuous representations for words.\n",
        "[Word embeddings](https://en.wikipedia.org/wiki/Word_embedding) are mappings from a discrete\n",
        "space to real-valued vectors. Word embeddings might be trained with neural networks,\n",
        "either as a by-product of other tasks (e.g., language modeling, neural machine translation),\n",
        "or with networks designed specifically for the word embedding task.\n",
        "\n",
        "Two problems associated with training neural word embeddings are related to the speed of training:\n",
        "(a) large volume of data, on which the network has to be trained on, and (b) time required to compute\n",
        "output probability distribution over large vocabularities.\n",
        "\n",
        "One of the most popular architectures for training word embeddings is called Word2vec [[1]()], [[2]()]. It builds on the idea that semantics of a word can be defined through the contexts,\n",
        "in which the word appears in the sentence.\n",
        "\n",
        "Let $w_1, w_2,\\ldots,w_N$ be an $N$-word sentence in a natural language.\n",
        "We define a context of a word $w_l$ a list of $n$ preceeding and following words\n",
        "$w_{l-n},\\ldots,w_{l-1},w_{l+1},\\dots,w_{l+n}$.\n",
        "\n",
        "The underlying assumption is that similar words appear in similar contexts.\n",
        "For instance, words *Poland* and *Monaco* are similar in a sense, that they are singular nouns\n",
        "describing abstract concepts of existing, european countries.\n",
        "We can convince ourselves by looking at exceprts from Wikipedia articles\n",
        "on Poland and Monaco:\n",
        "\n",
        "* Despite **Monaco's independence** and separate foreign policy\n",
        "* aimed to preserve **Poland's independence** and the szlachta's\n",
        "\n",
        "* **Monaco joined the** Council of Europe in 2004\n",
        "* **Poland joined the** Schengen Area in 2007\n",
        "\n",
        "* nearly one-fifth **of Poland's population** â€“ half of them\n",
        "* Christians comprise a total of 83.2% **of Monaco's population**.\n",
        "\n",
        "### Tasks\n",
        "You will use word vectors pre-computed on a large dataset.\n",
        "1. **[1p]** It has been observed, that word embeddings allow to perform semantic arithmetic where, for instance\n",
        "\n",
        "    **king** - **man** + **woman** ~= **queen**\n",
        "\n",
        "    This *analogy* task is often used as a quality measure of word embeddings. Load word embeddings and compute\n",
        "    their analogy score on a dataset of analogous pairs, expressed as an accuracy of accuracy of predicting a pair\n",
        "    item (**queen** in the example above). Specifically, compare `FastText` and `Word2vec` word embeddings.\n",
        "    \n",
        "2. **[1p]** Word embedding capture approximate semantics. Under an assumption that words of similar semantics\n",
        "    exist in different languages, a mapping $W: \\mathbb{R}^{300}\\mapsto\\mathbb{R}^{300}$ might be constructed that\n",
        "    translates word embeddings between languages. It has been shown that such ortonormal mappings allow to express\n",
        "    approximate, bilingual dictionaries. In addition, non-linear mappings do not offer additional benefits.\n",
        "\n",
        "    Given a simple English-Polish dictionary of word pairs (sourced from Wikitionary)\n",
        "    find an orthonormal mapping $W$ between English and Polish `FastText`\n",
        "    word embeddings using Procrustes analysis.\n",
        "\n",
        "3. **[1p]** Word embeddings can often be nicely visualized.\n",
        "    Make a 2-D `PCA` plot of word embeddings for countries and their capital cities\n",
        "    for `FastText` or `Word2vec`. Connect each country with its capital city with a line segment.\n",
        "    Can you see any regularities?\n",
        "    \n",
        "4. **[1p]** Plot 400 roughly most frequent words' embeddings (either `FastText` or `Word2vec`) in 2-D with `PCA`.\n",
        "    Skip stop words, punctuations, artifact words, etc. You can be imprecise and use heuristics\n",
        "    (e.g., select words than are at lest 3 charactes long).\n",
        "    Can you see any regularities? Another method of making meaningful visualizations is `t-SNE`.\n",
        "    \n",
        "    Make another 2-D visualization, this time using `t-SNE`. Visualizations with `t-SNE` are obtained\n",
        "    with gradient descent. Try to tweak optimization parameters to get lower optimization error,\n",
        "    than the one with default parameters.\n",
        "    Can you see any regularities this time?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5PkZy3FuqRr"
      },
      "source": [
        "# Problem 2: kNN Talker [11p+]\n",
        "\n",
        "In this exercise you will build a simple chatbot that uses distributed representations of words and sentences to perform a nearest neighbor selection of responses.\n",
        "\n",
        "We have collected two sets of data:\n",
        "- `./reddit_pairs.txt` of excerpts of [Reddit](https://www.reddit.com/) conversations,\n",
        "- `./hackernews_pairs.txt` of excertps from [Hackernews](https://news.ycombinator.com/).\n",
        "\n",
        "The two corpuses are formatted as `tab`-separated pairs of utterances: a `prompt` and a `response`. Successive lines belong to different conversations.\n",
        "\n",
        "The main idea of the chatbot is to build a representation of the user `input` and of all `prompts` from the corpus. Then select the best (or randomly one of the top few) matches and print the associated `response`.\n",
        "\n",
        "The key to get the bot working is to create good sentence representations. We will try:\n",
        "- averaging word embeddings\n",
        "- using sentence models such as BERT.\n",
        "\n",
        "### Warning:\n",
        "The Reddit corpus may contain abusive language, it was not heavily cleaned.\n",
        "\n",
        "### Tasks\n",
        "The code below is a starting point, but you can develop you own. The following list suggests some actions to try, along with the points that reflect our subjective hardness. The exercise is worth 6 regular points (i.e. the report, word embedding-based model and BERT-based model), anything on top of it will be counted as a bonus.\n",
        "\n",
        "1. [2p] Type in a Markdown cell a report of your actions, what did you try, why, what was the result. Show exemplary conversations (they must be probable under your model). Cherry-pick 3 nice dialogues.\n",
        "2. [2p] Represent sentences by averaging their word vectors. Properly handle tokenization (you can use regular expressions or e.g. `nltk` library). Describe how you handle lower and upper cased words. Try a few nearest neighbor selection methods (such as euclidean or cosine distance). See how embedding normalization affects the results (you can normalize individual word vectors, full sentence vectors etc.).\n",
        "3. [2p] Use the [transformers](https://huggingface.co/transformers) package to load a pretrained BERT model. Use it to represent sentences by e.g. averaging the activations in the last layer.\n",
        "4. [1p] Incoportate context: keep a running average of past conversation turns.\n",
        "5. [1p] Do data cleaning (including profanieties), finding rules for good responses.\n",
        "6. [1p] Try mixing different sentence representation techniques.\n",
        "7. [2p] Try to cluster responses to the highest scored prompts. Which responses are more funny: from the largerst or from the smallest clusters?.\n",
        "8. [1p+] Implement your own enhancements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3SN5za0rEdp"
      },
      "source": [
        "# Starter code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czZXkkVEASTf"
      },
      "source": [
        "# Please note that this code needs only to be run in a fresh runtime.\n",
        "# However, it can be rerun afterwards too.\n",
        "\n",
        "# Download word vectors\n",
        "!pip install -q gdown httpimport\n",
        "![ -e word2vec.tar.xz ] || gdown 'https://drive.google.com/uc?id=1v6D8IjYVFlonhQuN_J3PML5KSVQSpbED' -O word2vec.tar.xz\n",
        "![ -d word2vec ] || tar Jxf word2vec.tar.xz\n",
        "\n",
        "# Download conversation corpuses\n",
        "![ -e  hackernews_pairs.txt ] || gdown 'https://drive.google.com/uc?id=10cp2maNp1suzc5BaFQwDJr2GTKXHQOz_' -O hackernews_pairs.txt\n",
        "![ -e  reddit_pairs.txt ] || gdown 'https://drive.google.com/uc?id=1Uf0Xl9aqQVBBpOwhYTV7iWCwj95FDqtL' -O reddit_pairs.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJSP8qDQtI8H"
      },
      "source": [
        "# Huggingface Transformers implementation\n",
        "!pip install -q tqdm boto3 requests regex sentencepiece sacremoses\n",
        "!pip install -q transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky2fLuhOuqQN"
      },
      "source": [
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF7Xu9Z1GBk6"
      },
      "source": [
        "import collections\n",
        "\n",
        "import codecs\n",
        "\n",
        "import numpy as np\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "from scipy.spatial import distance\n",
        "\n",
        "import nltk\n",
        "import sklearn\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pprint\n",
        "import gensim\n",
        "\n",
        "import io\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "import tabulate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCuRj_u_uqQY"
      },
      "source": [
        "## 1.1: Analogies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L11W4J6LtIgw"
      },
      "source": [
        "# Word embeddings assign a vector to each word. To use them we need three things:\n",
        "# 1. the embeddings themselves\n",
        "# 2. a dictionary from words to their embedding ids\n",
        "# 3. an inverse dictionary\n",
        "\n",
        "Embedding = collections.namedtuple(\n",
        "    'Embedding',\n",
        "    ['vec', 'word2idx', 'idx2word'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwb_QRNiuqQZ"
      },
      "source": [
        "def load_vecs_npy(base_path):\n",
        "    \"\"\"Load small embeddings in .npy format.\"\"\"\n",
        "    vec = np.load(base_path + '.npy')\n",
        "    idx2word = [l.strip() for l in codecs.open(\n",
        "                      base_path + '.txt', 'r', 'utf-8')]\n",
        "    word2idx = {w:i for (i,w) in enumerate(idx2word)}\n",
        "    return Embedding(vec, word2idx, idx2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5UHEt8kuqQc"
      },
      "source": [
        "# Load top 200k word embeddings: Word2vec and FastText\n",
        "word2vec = load_vecs_npy('word2vec/word2vec_GoogleNews_200k')\n",
        "ftext = load_vecs_npy('word2vec/fasttext_wikien_200k')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF_7SHHJ5CWY"
      },
      "source": [
        "def load_analogies():\n",
        "    '''Load tuples of analogies, e.g., (man, woman, king, queen)'''\n",
        "    questions_path = 'word2vec/questions-words.txt'\n",
        "    analogies = [l.strip().split() for l in open(questions_path, 'r') \\\n",
        "                 if not l.startswith(':')]\n",
        "    return analogies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJATnKFVuqQg"
      },
      "source": [
        "def eval_analogies(vecs, analogies):\n",
        "    \"\"\"\n",
        "    Compute the accuracy on the analogy task.\n",
        "    \n",
        "    In the task, quadruples of words are given (q1, q2, r1, r2).\n",
        "    The words q1 and q2 are bound by a relation. The words r1 and r2 \n",
        "    are bound by the same relation. The task is to predict r2 knowing words\n",
        "    q1, q2, and r1.\n",
        "    \n",
        "    Example:\n",
        "    Quadruple [King, Queen, Man, Woman] yields the question\n",
        "    King is to Queen as Man is to ????\n",
        "    \n",
        "    Args:\n",
        "        word_vecs: Embeddings tupes\n",
        "        analogies: list of quadruples: (q1, q2, r1, r2)\n",
        "        \n",
        "    Returns:\n",
        "        percentage of correct answers\n",
        "    \"\"\"\n",
        "    indexes = [[vecs.word2idx.get(w, None) for w in tupl] \\\n",
        "                for tupl in analogies]\n",
        "    indexes = [tupl for tupl in indexes \\\n",
        "               if all([v is not None for v in tupl])]\n",
        "    # indexes holds quadruples of ints giving the ids of words from our vocab.\n",
        "    indexes = np.asarray(indexes)\n",
        "    print('Got vocab for %d/%d pairs' % (indexes.shape[0], len(analogies)))\n",
        "    \n",
        "    # Extract the vectors for the query words\n",
        "    q1 = vecs.vec[indexes[:, 0]]\n",
        "    q2 = vecs.vec[indexes[:, 1]]\n",
        "    r1 = vecs.vec[indexes[:, 2]]\n",
        "    \n",
        "    # Extractr the word IDs for the correct answers\n",
        "    r2_inds = indexes[:, 3]\n",
        "\n",
        "    #\n",
        "    # TODO \n",
        "    #\n",
        "    # Compute the approximate location of word r2 as r2 = r1 + (q2 - q1)\n",
        "    # Find the word closest to this location using cosine distance.\n",
        "    # Return it's id and compute the accurracy.\n",
        "    #\n",
        "\n",
        "    r2_pred = TODO\n",
        "    \n",
        "    \n",
        "    # Normalize length and compute dot product between r2_pred and word_vecs\n",
        "    # to get cosine distance\n",
        "    r2_pred_norm = TODO\n",
        "    vecs_norm = TODO\n",
        "    \n",
        "    # Compute in chunks to save memory\n",
        "    r2_pred_inds = np.concatenate([np.argmax(r2_pred_norm[i:i+1000].dot(vecs_norm.T), axis=1) \\\n",
        "                                   for i in range(0, r2_pred.shape[0], 1000)])\n",
        "    return 100.0 * (r2_pred_inds == r2_inds).sum() / r2_inds.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl0JtuxSuqQj"
      },
      "source": [
        "# Load analogy tuples, e.g., (man, woman, king, queen)\n",
        "analogies = load_analogies()\n",
        "\n",
        "# Some are uppercased geographical names (and FastTexts are lowercased)\n",
        "analogies_lower = [[w.lower() for w in tupl] for tupl in analogies]\n",
        "\n",
        "print(analogies[0])\n",
        "print(analogies_lower[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44kDHN16uqQn"
      },
      "source": [
        "# Keep in mind that the vocab is restricted to 200k most freq words\n",
        "# (in the training corpus!)\n",
        "print('FastText analogy task accuracy:')\n",
        "print('-------------------------------')\n",
        "# Fast-text was trained on lowercased text only\n",
        "print(eval_analogies(ftext, analogies_lower), '% correct')\n",
        "\n",
        "print('\\nWord2vec analogy task accuracy:')\n",
        "print('-------------------------------')\n",
        "# Word2vec has case information\n",
        "print(eval_analogies(word2vec, analogies), '% correct')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTNt90qyuqQr"
      },
      "source": [
        "## 1.2: translation through alignment of vector spaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie4nhd8ZuqQt"
      },
      "source": [
        "# We're need Polish embeddings\n",
        "ftext_pl = load_vecs_npy('word2vec/fasttext_wikipl_200k')\n",
        "\n",
        "# Load a simple wikitionary-based dict of word pairs\n",
        "en_pl = [l.strip().split('\\t') for l in codecs.open(\n",
        "    'word2vec/word2vec_en_pl', 'r', 'utf-8') if not '<UNK>' in l]\n",
        "en_pl = {t[0]:t[1] for t in en_pl if len(t) == 2}\n",
        "\n",
        "# Keep those, for which we have embeddings\n",
        "en_pl = {en:pl for (en,pl) in en_pl.items() \\\n",
        "         if en in ftext.word2idx and pl in ftext_pl.word2idx}\n",
        "print('Dictionary size:', len(en_pl))\n",
        "print('good --', en_pl['good'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8XGFZp3uqQz"
      },
      "source": [
        "# Select word embeddings for dictionary words\n",
        "en_words = sorted(en_pl.keys())\n",
        "V_en = ftext.vec[[ftext.word2idx[w] for w in en_words]]\n",
        "V_pl = ftext_pl.vec[[ftext_pl.word2idx[en_pl[w]] for w in en_words]]\n",
        "print(V_en.shape, V_pl.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIF8guv6uqQ1"
      },
      "source": [
        "# Find an orthogonal transformation from V_en to V_pl.\n",
        "# which minimizes square reconstruction error\n",
        "W = orthogonal_procrustes(V_en, V_pl)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yReVugjxuqQ5"
      },
      "source": [
        "def translate(W, v, vecs2):\n",
        "    #\n",
        "    # TODO\n",
        "    #\n",
        "    # Transform source word vector v using W getting a location in target space.\n",
        "    # Find the closest word in target space using the cosine distance.\n",
        "    #\n",
        "    \n",
        "    return vecs2.idx2word[idx]\n",
        "\n",
        "# Select random, fairly frequent words not from the dictionary\n",
        "tr_words = [i for i in np.random.randint(1000, 2000, 100) \\\n",
        "            if ftext.idx2word[i] not in en_pl]\n",
        "tr_words = tr_words[:20]\n",
        "\n",
        "rows = [[ftext.idx2word[i], translate(W, ftext.vec[i], ftext_pl)] \\\n",
        "         for i in tr_words] \n",
        "print(tabulate.tabulate(rows))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc1X6ONsuqQ9"
      },
      "source": [
        "## 1.3: PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFXt5nxRuqQ-"
      },
      "source": [
        "capitals = [l.strip().split('\\t') for l in codecs.open('word2vec/countries_capitals', 'r', 'utf-8')]\n",
        "capitals = {country:capital for (country,capital) in capitals}\n",
        "\n",
        "# Select those present in Word2vec vocab\n",
        "capitals = {k:v for (k,v) in capitals.items() \\\n",
        "            if k in word2vec.word2idx and v in word2vec.word2idx}\n",
        "\n",
        "# Flatten the array to have a list of [country, capital, country, capital, ...]\n",
        "geo = [e for pair in capitals.items() for e in pair]\n",
        "print(len(geo) // 2, 'pairs', geo[:6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_laByxLuqRD"
      },
      "source": [
        "geo_inds = [word2vec.word2idx[w] for w in geo]\n",
        "geo_vecs = word2vec.vec[geo_inds]\n",
        "print(geo_vecs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE1HZ742uqRH"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#\n",
        "# TODO\n",
        "# use PCA from sklearn.decomposition to project the countries and capitals into 2D.\n",
        "# draw lines connecting each country with its capital\n",
        "#\n",
        "# Hint:\n",
        "# - the function \"annotate\" can be used to put text onto the plot\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgbSODUOuqRM"
      },
      "source": [
        "## 1.4: PCA vs t-SNE on frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoBJkQLKuqRP"
      },
      "source": [
        "# Select words starting from the 200th to ommit stop words,\n",
        "# which have at least 3 chars\n",
        "top_words = [w for (i,w) in enumerate(ftext.idx2word) \\\n",
        "             if i > 200 and len(w) >= 3][:400]\n",
        "top_inds = [ftext.word2idx[w] for w in top_words]\n",
        "\n",
        "\n",
        "#\n",
        "# TODO: make a 2D PCA projection of the selected words.\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nHVjV6NuqRV"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "#\n",
        "# TODO: make a 2D t-SNE projection of the selected words.\n",
        "# Things will cluster much nicer\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxF70pAHJ2bR"
      },
      "source": [
        "# TODO: zomm in on 3 meaningful clusters\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RLhJIkdqvyT"
      },
      "source": [
        "## Problem 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF8EUDVzwrHr"
      },
      "source": [
        "prompts = []\n",
        "responses = []\n",
        "err_lines = []\n",
        "with open('./hackernews_pairs.txt') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        line = line.split('\\t')\n",
        "        if len(line)!=2:\n",
        "            err_lines.append(line)\n",
        "        else:\n",
        "            prompts.append(line[0])\n",
        "            responses.append(line[1])\n",
        "print(f\"Failed to parse the following {len(err_lines)} lines: {err_lines}\") \n",
        "print(f\"Sample fialogue pairs: \\n{pprint.pformat(list(zip(prompts[:15], responses)))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTiTRTs_2i3f"
      },
      "source": [
        "class BasicEncoder:\n",
        "    def encode(self, sentence):\n",
        "        # this is a base class!\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def encode_corpus(self, sentences):\n",
        "        ret = [self.encode(sentence) for sentence in tqdm(sentences)]\n",
        "        return np.vstack(ret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x08LL_eT9u3B"
      },
      "source": [
        "The code below implements a simple baseline: we encode each sentence as a sparse vector which sums word occurrences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEmX8nTy4fvg"
      },
      "source": [
        "class OneHotEncoder(BasicEncoder):\n",
        "    def __init__(self, sentences):\n",
        "        self.vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
        "        self.vectorizer.fit(sentences)\n",
        "    \n",
        "    def encode(self, sentence):\n",
        "        return self.vectorizer.transform([sentence])[0]\n",
        "\n",
        "    def encode_corpus(self, sentences):\n",
        "        # Override because sklearn already works on batches\n",
        "        encodings = self.vectorizer.transform(sentences)\n",
        "        # Note: this code needs to handl the scipy sparse matrix\n",
        "        # which has subtle differences with numpy ndarrays\n",
        "        norms = np.array((encodings.power(2)).sum(1))**0.5\n",
        "        encodings = encodings.multiply(1.0 / norms)\n",
        "        return encodings\n",
        "\n",
        "encoder = OneHotEncoder(prompts)\n",
        "encodings = encoder.encode_corpus(prompts)\n",
        "\n",
        "prompt = \"Ultimate question: Windows or Linux?\"\n",
        "enc = encoder.encode(prompt)\n",
        "\n",
        "# Deal with encodings being sparse matrices. Word2vecs will not have the pecularities\n",
        "scores = (encodings @ enc.T).toarray().ravel()\n",
        "top_idx = scores.argsort()[-10:][::-1]\n",
        "\n",
        "for idx in top_idx:\n",
        "    print(scores[idx], prompts[idx], ':', responses[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nu0JqmPKcm8"
      },
      "source": [
        "# TODO: build a simple dialogue system using these k-neares neighbor matches, \n",
        "# perform a few test conversations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOYvQnpBwvvG"
      },
      "source": [
        "class Word2VecEncoder(BasicEncoder):\n",
        "    def __init__(self, vecs):\n",
        "        self._vecs = vecs\n",
        "\n",
        "    def _get_vec(self, word):\n",
        "        # Find the vector for word, or use a suitbale out-of-vocabulary vector\n",
        "        # For extra points try backing off to a lowercased dictionary\n",
        "        \n",
        "        return 0\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        ret = np.zeros(self._vecs.vec.shape[1])\n",
        "        for token in nltk.tokenize.word_tokenize(sentence):\n",
        "            word_vec = self._get_vec(token)\n",
        "            ret += word_vec\n",
        "        ret /= (np.linalg.norm(ret) + 1e-5)\n",
        "        return ret\n",
        "\n",
        "encoder = Word2VecEncoder(word2vec)\n",
        "encodings = encoder.encode_corpus(prompts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfzUJDJJ7Ric"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}